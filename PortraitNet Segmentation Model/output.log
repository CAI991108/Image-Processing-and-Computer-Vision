(pytorch) cai@cai-ROG-Strix:~/Downloads/test_shi/myTrain$ python test.py --batch=32 > output.log
2024-10-16 22:40:41.141498: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-10-16 22:40:41.142863: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-10-16 22:40:41.160754: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-10-16 22:40:41.160775: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-10-16 22:40:41.160791: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-16 22:40:41.164333: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-10-16 22:40:41.820249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-10-16 22:40:41.821036: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
/home/cai/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

===========> loading config <============
config path:  /home/cai/Downloads/test_shi/config/model_mobilenetv2_with_two_auxiliary_losses.yaml
===========> loading data <===========
image number in training:  1447
image number in testing:  289
finish load dataset ...
===========> loading model <===========
finish load PortraitNet ...
===========>   training    <===========
Epoch: [0][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 4.8739 (4.8739)	
0.99948967 1.3270636e-26
===========>   testing    <===========
Epoch: [0][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.6146 (0.6146)	
0.9997873 1.3274175e-08
Epoch: [0][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.6201 (0.6374)	
0.99977535 2.576378e-06
Epoch: [0][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.6401 (0.6365)	
0.99971396 2.6805774e-08
loss:  0.5011127616491449 10000
===========>   training    <===========
Epoch: [1][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 1.4120 (1.4120)	
0.99973243 5.5074725e-07
===========>   testing    <===========
Epoch: [1][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4500 (0.4500)	
0.98652816 4.944057e-07
Epoch: [1][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4696 (0.5684)	
0.9890387 3.4168585e-07
Epoch: [1][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.6305 (0.5682)	
0.9853522 2.9821695e-06
loss:  0.41267207185660804 0.5011127616491449
===========>   training    <===========
Epoch: [2][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 1.1536 (1.1536)	
0.99191827 1.3161732e-08
===========>   testing    <===========
Epoch: [2][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4358 (0.4358)	
0.93349904 5.9670384e-07
Epoch: [2][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4494 (0.4569)	
0.93926966 5.7475967e-07
Epoch: [2][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4800 (0.4583)	
0.9317452 4.132304e-06
loss:  0.35666999687929724 0.41267207185660804
===========>   training    <===========
Epoch: [3][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.9106 (0.9106)	
0.9910958 1.6904262e-07
===========>   testing    <===========
Epoch: [3][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.3705 (0.3705)	
0.91727716 0.00014085676
Epoch: [3][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4808 (0.3728)	
0.9272867 0.0006300539
Epoch: [3][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.3818 (0.3763)	
0.9182275 0.00012618871
loss:  0.26946507989347535 0.35666999687929724
===========>   training    <===========
Epoch: [4][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.8161 (0.8161)	
0.94797647 0.00084820157
===========>   testing    <===========
Epoch: [4][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4576 (0.4576)	
0.9290866 2.2777765e-05
Epoch: [4][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4778 (0.3412)	
0.9167584 0.00011601965
Epoch: [4][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.3049 (0.3389)	
0.9290267 7.9739446e-05
loss:  0.2722562394637539 0.26946507989347535
===========>   training    <===========
Epoch: [5][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.6822 (0.6822)	
0.9456341 0.0058767092
===========>   testing    <===========
Epoch: [5][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.3354 (0.3354)	
0.94616914 0.0004389311
Epoch: [5][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.5151 (0.3407)	
0.938964 0.00021540404
Epoch: [5][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.3032 (0.3414)	
0.9460172 0.00063899404
loss:  0.24673759044060473 0.26946507989347535
===========>   training    <===========
Epoch: [6][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.6793 (0.6793)	
0.9473362 0.00039262636
===========>   testing    <===========
Epoch: [6][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.3313 (0.3313)	
0.96274114 0.00013709516
Epoch: [6][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.5358 (0.3067)	
0.9452436 8.912953e-05
Epoch: [6][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2958 (0.3079)	
0.9621511 7.5422926e-05
loss:  0.23942384772070469 0.24673759044060473
===========>   training    <===========
Epoch: [7][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.7631 (0.7631)	
0.9622094 9.800877e-07
===========>   testing    <===========
Epoch: [7][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.3422 (0.3422)	
0.9703128 8.8584784e-05
Epoch: [7][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.5057 (0.2933)	
0.9621726 0.00017127405
Epoch: [7][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2830 (0.2873)	
0.97016776 5.5990862e-05
loss:  0.22786815602479127 0.23942384772070469
===========>   training    <===========
Epoch: [8][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.5992 (0.5992)	
0.97225744 0.0004375738
===========>   testing    <===========
Epoch: [8][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2797 (0.2797)	
0.97431993 0.00024881892
Epoch: [8][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4907 (0.3093)	
0.9701357 0.00022110902
Epoch: [8][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.3037 (0.3134)	
0.9742357 0.00010123183
loss:  0.23619180653755678 0.22786815602479127
===========>   training    <===========
Epoch: [9][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.5656 (0.5656)	
0.97440934 3.0502351e-06
===========>   testing    <===========
Epoch: [9][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.3508 (0.3508)	
0.9770785 6.1370424e-06
Epoch: [9][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.5121 (0.2687)	
0.9671047 4.5986846e-05
Epoch: [9][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2579 (0.2616)	
0.9765879 7.5046573e-06
loss:  0.22208649873783137 0.22786815602479127
===========>   training    <===========
Epoch: [10][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.6251 (0.6251)	
0.9730498 1.3680337e-05
===========>   testing    <===========
Epoch: [10][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2560 (0.2560)	
0.97990304 0.00018773906
Epoch: [10][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.5170 (0.2470)	
0.9764301 0.00024407846
Epoch: [10][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2391 (0.2465)	
0.9798069 0.000118711876
loss:  0.19297535270717714 0.22208649873783137
===========>   training    <===========
Epoch: [11][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.5148 (0.5148)	
0.9806044 4.769948e-05
===========>   testing    <===========
Epoch: [11][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2937 (0.2937)	
0.9891985 0.0001958061
Epoch: [11][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.5506 (0.2561)	
0.9835369 0.00031731874
Epoch: [11][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2491 (0.2602)	
0.9892379 5.938814e-05
loss:  0.2009686860680021 0.19297535270717714
===========>   training    <===========
Epoch: [12][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.5253 (0.5253)	
0.9883504 0.00033161324
===========>   testing    <===========
Epoch: [12][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2076 (0.2076)	
0.9854071 3.5234134e-05
Epoch: [12][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.6518 (0.2564)	
0.98118883 9.7714656e-05
Epoch: [12][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2294 (0.2662)	
0.9854865 1.5949072e-05
loss:  0.20380566694754143 0.19297535270717714
===========>   training    <===========
Epoch: [13][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.6164 (0.6164)	
0.9850305 0.00018126161
===========>   testing    <===========
Epoch: [13][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4017 (0.4017)	
0.9921822 5.567858e-06
Epoch: [13][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.6007 (0.2372)	
0.9850889 3.1998192e-05
Epoch: [13][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2483 (0.2326)	
0.99166834 1.1840854e-06
loss:  0.1962333697587194 0.19297535270717714
===========>   training    <===========
Epoch: [14][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4835 (0.4835)	
0.9908857 2.876442e-05
===========>   testing    <===========
Epoch: [14][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2345 (0.2345)	
0.98787326 0.00034994294
Epoch: [14][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.3368 (0.2262)	
0.977857 0.00025605087
Epoch: [14][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.1969 (0.2306)	
0.98792285 8.871176e-05
loss:  0.17325286806983098 0.19297535270717714
===========>   training    <===========
Epoch: [15][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.5567 (0.5567)	
0.9880151 1.3144696e-05
===========>   testing    <===========
Epoch: [15][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2097 (0.2097)	
0.9920426 0.00011874233
Epoch: [15][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.3411 (0.2220)	
0.98612034 0.00025708758
Epoch: [15][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2043 (0.2257)	
0.9920642 2.9204619e-05
loss:  0.17519737581859296 0.17325286806983098
===========>   training    <===========
Epoch: [16][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.5059 (0.5059)	
0.9923535 8.200178e-05
===========>   testing    <===========
Epoch: [16][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2166 (0.2166)	
0.99152035 8.19398e-05
Epoch: [16][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4915 (0.2456)	
0.98996556 7.972377e-05
Epoch: [16][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2008 (0.2543)	
0.991638 1.655921e-05
loss:  0.19185614498351422 0.17325286806983098
===========>   training    <===========
Epoch: [17][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.7213 (0.7213)	
0.9915025 7.460223e-06
===========>   testing    <===========
Epoch: [17][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.1718 (0.1718)	
0.99381566 2.7953385e-05
Epoch: [17][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4320 (0.2064)	
0.98989385 3.015257e-05
Epoch: [17][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2133 (0.2062)	
0.9936446 5.5626037e-06
loss:  0.1659253926392682 0.17325286806983098
===========>   training    <===========
Epoch: [18][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.5083 (0.5083)	
0.9935494 4.331421e-05
===========>   testing    <===========
Epoch: [18][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.1595 (0.1595)	
0.9954007 8.315484e-05
Epoch: [18][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.2896 (0.2008)	
0.99029493 6.025995e-05
Epoch: [18][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.1736 (0.2013)	
0.9955224 4.6650814e-05
loss:  0.1570768206354486 0.1659253926392682
===========>   training    <===========
Epoch: [19][0/46]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.4458 (0.4458)	
0.9965648 2.9946845e-05
===========>   testing    <===========
Epoch: [19][0/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.1742 (0.1742)	
0.99220896 8.434471e-05
Epoch: [19][100/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.3404 (0.2108)	
0.9849325 7.4816824e-05
Epoch: [19][200/289]	Lr-deconv: [0.0]	Lr-other: [0.001]	Loss 0.1685 (0.2094)	
0.9921858 2.8989185e-05
loss:  0.1651120610309773 0.1570768206354486
===========>   training    <===========
Epoch: [20][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.4725 (0.4725)	
0.99155265 0.000116998075
===========>   testing    <===========
Epoch: [20][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1900 (0.1900)	
0.99589515 4.7893736e-05
Epoch: [20][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.5454 (0.1971)	
0.99357504 1.705107e-05
Epoch: [20][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1382 (0.1964)	
0.9958846 4.97813e-06
loss:  0.15644103714851876 0.1570768206354486
===========>   training    <===========
Epoch: [21][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.3575 (0.3575)	
0.99610496 3.5740235e-05
===========>   testing    <===========
Epoch: [21][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1486 (0.1486)	
0.99703646 9.0214555e-05
Epoch: [21][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.3782 (0.1905)	
0.99549437 4.7506725e-05
Epoch: [21][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1324 (0.1929)	
0.9970771 2.157362e-06
loss:  0.15065778391939677 0.15644103714851876
===========>   training    <===========
Epoch: [22][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.4329 (0.4329)	
0.9967834 1.8488396e-05
===========>   testing    <===========
Epoch: [22][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1450 (0.1450)	
0.99531 1.546922e-06
Epoch: [22][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.5010 (0.1886)	
0.9857702 1.2123443e-05
Epoch: [22][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1580 (0.1826)	
0.995129 1.6873241e-07
loss:  0.1503914779498039 0.15065778391939677
===========>   training    <===========
Epoch: [23][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.4173 (0.4173)	
0.99599516 4.005084e-07
===========>   testing    <===========
Epoch: [23][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.2284 (0.2284)	
0.9967236 0.00010581512
Epoch: [23][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.3378 (0.1885)	
0.9948579 4.3154607e-05
Epoch: [23][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1879 (0.1870)	
0.9966798 1.8897337e-05
loss:  0.14425017561121767 0.1503914779498039
===========>   training    <===========
Epoch: [24][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.3985 (0.3985)	
0.99755585 1.970532e-05
===========>   testing    <===========
Epoch: [24][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1329 (0.1329)	
0.9970298 3.2382773e-05
Epoch: [24][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.2779 (0.1648)	
0.9900023 3.3362987e-05
Epoch: [24][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1470 (0.1652)	
0.9969919 3.0340004e-06
loss:  0.12864694092724593 0.14425017561121767
===========>   training    <===========
Epoch: [25][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.3750 (0.3750)	
0.99723405 6.270252e-06
===========>   testing    <===========
Epoch: [25][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1574 (0.1574)	
0.997988 1.9393548e-05
Epoch: [25][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1813 (0.1890)	
0.9963626 4.7452395e-05
Epoch: [25][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1703 (0.1907)	
0.9979241 3.7874263e-06
loss:  0.143590914883857 0.12864694092724593
===========>   training    <===========
Epoch: [26][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.5415 (0.5415)	
0.9973605 0.00010068384
===========>   testing    <===========
Epoch: [26][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.2669 (0.2669)	
0.9970822 9.89469e-06
Epoch: [26][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.2069 (0.1580)	
0.99501836 2.1098896e-05
Epoch: [26][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1300 (0.1528)	
0.996716 9.1387426e-07
loss:  0.12379262470023256 0.12864694092724593
===========>   training    <===========
Epoch: [27][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.4072 (0.4072)	
0.9963198 5.5068763e-06
===========>   testing    <===========
Epoch: [27][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1671 (0.1671)	
0.9982614 7.4168584e-06
Epoch: [27][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.2654 (0.1672)	
0.9970994 3.773758e-05
Epoch: [27][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1508 (0.1605)	
0.9981353 5.641807e-06
loss:  0.12471337733793086 0.12379262470023256
===========>   training    <===========
Epoch: [28][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.3824 (0.3824)	
0.9981199 7.1359027e-06
===========>   testing    <===========
Epoch: [28][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1302 (0.1302)	
0.99820065 1.7882565e-05
Epoch: [28][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.2479 (0.1602)	
0.9812651 2.2632561e-05
Epoch: [28][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1396 (0.1602)	
0.9978271 4.5437214e-06
loss:  0.12779202477060425 0.12379262470023256
===========>   training    <===========
Epoch: [29][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.3182 (0.3182)	
0.9982039 0.00021777728
===========>   testing    <===========
Epoch: [29][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.2595 (0.2595)	
0.99810994 1.7082983e-06
Epoch: [29][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1219 (0.1594)	
0.99602616 5.7287016e-06
Epoch: [29][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1231 (0.1540)	
0.99810314 4.909439e-07
loss:  0.12195207173039457 0.12379262470023256
===========>   training    <===========
Epoch: [30][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.2997 (0.2997)	
0.9978911 1.2221279e-06
===========>   testing    <===========
Epoch: [30][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1851 (0.1851)	
0.9988977 1.3490575e-05
Epoch: [30][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1021 (0.1454)	
0.9981323 7.0295937e-06
Epoch: [30][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1218 (0.1407)	
0.9989097 2.8782765e-06
loss:  0.11288750741906628 0.12195207173039457
===========>   training    <===========
Epoch: [31][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.4256 (0.4256)	
0.99846125 6.335548e-06
===========>   testing    <===========
Epoch: [31][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.2668 (0.2668)	
0.9980404 3.4551551e-06
Epoch: [31][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.2439 (0.1570)	
0.9960114 7.736368e-06
Epoch: [31][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1313 (0.1541)	
0.99791247 6.230199e-07
loss:  0.12196440984995871 0.11288750741906628
===========>   training    <===========
Epoch: [32][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.3713 (0.3713)	
0.9979113 3.6280308e-05
===========>   testing    <===========
Epoch: [32][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1627 (0.1627)	
0.9987141 1.3186983e-05
Epoch: [32][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1503 (0.1468)	
0.9983516 2.7494048e-05
Epoch: [32][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1570 (0.1428)	
0.9986834 8.321744e-06
loss:  0.11354086023033183 0.11288750741906628
===========>   training    <===========
Epoch: [33][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.3817 (0.3817)	
0.999033 3.4225502e-06
===========>   testing    <===========
Epoch: [33][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.2960 (0.2960)	
0.9980076 5.12929e-06
Epoch: [33][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.0919 (0.1716)	
0.9976057 3.471531e-06
Epoch: [33][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.2033 (0.1682)	
0.99805295 2.7195808e-06
loss:  0.13084700917655945 0.11288750741906628
===========>   training    <===========
Epoch: [34][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.3629 (0.3629)	
0.99815303 7.55976e-06
===========>   testing    <===========
Epoch: [34][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1635 (0.1635)	
0.9986066 1.1422521e-06
Epoch: [34][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.2704 (0.1375)	
0.9967257 5.371759e-06
Epoch: [34][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1061 (0.1344)	
0.99847335 7.98196e-07
loss:  0.10713984396144749 0.11288750741906628
===========>   training    <===========
Epoch: [35][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.3610 (0.3610)	
0.9986873 1.5855216e-05
===========>   testing    <===========
Epoch: [35][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1691 (0.1691)	
0.9988356 5.2158857e-06
Epoch: [35][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1023 (0.1537)	
0.9985896 9.000578e-06
Epoch: [35][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1934 (0.1494)	
0.99887437 4.231824e-06
loss:  0.11714544448783004 0.10713984396144749
===========>   training    <===========
Epoch: [36][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.4115 (0.4115)	
0.99850994 1.2026918e-06
===========>   testing    <===========
Epoch: [36][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1197 (0.1197)	
0.9991116 7.97439e-06
Epoch: [36][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1497 (0.1358)	
0.99872357 1.2189282e-05
Epoch: [36][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1352 (0.1369)	
0.9990577 3.1828963e-06
loss:  0.10789619616255697 0.10713984396144749
===========>   training    <===========
Epoch: [37][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.3492 (0.3492)	
0.9983645 1.746503e-07
===========>   testing    <===========
Epoch: [37][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1090 (0.1090)	
0.9987907 4.2323702e-08
Epoch: [37][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.0755 (0.1566)	
0.99634105 4.3759417e-07
Epoch: [37][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1756 (0.1512)	
0.9985667 6.901478e-08
loss:  0.11989117139281846 0.10713984396144749
===========>   training    <===========
Epoch: [38][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.3574 (0.3574)	
0.9988745 9.5454045e-05
===========>   testing    <===========
Epoch: [38][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1172 (0.1172)	
0.9987721 2.5150362e-06
Epoch: [38][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.0853 (0.1286)	
0.998439 5.6091434e-05
Epoch: [38][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1289 (0.1268)	
0.9988564 6.7343967e-06
loss:  0.10011432144313026 0.10713984396144749
===========>   training    <===========
Epoch: [39][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.4108 (0.4108)	
0.9982436 6.4436186e-05
===========>   testing    <===========
Epoch: [39][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1756 (0.1756)	
0.9991474 4.8296843e-06
Epoch: [39][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.0890 (0.1332)	
0.9989969 2.5432075e-06
Epoch: [39][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00095]	Loss 0.1565 (0.1322)	
0.9991798 3.1111344e-06
loss:  0.10436417686537047 0.10011432144313026
===========>   training    <===========
Epoch: [40][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.3772 (0.3772)	
0.99911016 1.7966266e-06
===========>   testing    <===========
Epoch: [40][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1067 (0.1067)	
0.9992551 9.110618e-06
Epoch: [40][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1357 (0.1378)	
0.99905735 1.6791319e-05
Epoch: [40][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1822 (0.1401)	
0.9992366 4.1193234e-06
loss:  0.11036931081293111 0.10011432144313026
===========>   training    <===========
Epoch: [41][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.3347 (0.3347)	
0.99921536 3.8274263e-07
===========>   testing    <===========
Epoch: [41][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1474 (0.1474)	
0.999196 5.489827e-07
Epoch: [41][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2239 (0.1347)	
0.99881566 3.221656e-06
Epoch: [41][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0985 (0.1345)	
0.9991578 7.105779e-07
loss:  0.10659546019953214 0.10011432144313026
===========>   training    <===========
Epoch: [42][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2868 (0.2868)	
0.99946195 7.641296e-06
===========>   testing    <===========
Epoch: [42][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1296 (0.1296)	
0.9989881 2.975255e-06
Epoch: [42][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0704 (0.1235)	
0.9984831 4.42845e-06
Epoch: [42][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1379 (0.1214)	
0.9991109 2.6623063e-06
loss:  0.09530955566413213 0.10011432144313026
===========>   training    <===========
Epoch: [43][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.3713 (0.3713)	
0.999153 3.1830282e-07
===========>   testing    <===========
Epoch: [43][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1259 (0.1259)	
0.9991603 4.653979e-06
Epoch: [43][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1465 (0.1172)	
0.9990289 5.3830204e-06
Epoch: [43][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0978 (0.1192)	
0.9991899 1.5404136e-06
loss:  0.0937453528470964 0.09530955566413213
===========>   training    <===========
Epoch: [44][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.3108 (0.3108)	
0.9991111 2.6963717e-06
===========>   testing    <===========
Epoch: [44][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1178 (0.1178)	
0.99927336 1.1338588e-07
Epoch: [44][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1386 (0.1397)	
0.998995 9.051122e-07
Epoch: [44][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2297 (0.1384)	
0.9992902 1.4085381e-07
loss:  0.11640087677418942 0.0937453528470964
===========>   training    <===========
Epoch: [45][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2835 (0.2835)	
0.9994019 2.6617427e-06
===========>   testing    <===========
Epoch: [45][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0928 (0.0928)	
0.9994344 3.3057248e-07
Epoch: [45][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2580 (0.1209)	
0.9992465 4.1625663e-06
Epoch: [45][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1257 (0.1209)	
0.99945766 6.727418e-07
loss:  0.0940093369652718 0.0937453528470964
===========>   training    <===========
Epoch: [46][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.3047 (0.3047)	
0.9994746 1.2297132e-06
===========>   testing    <===========
Epoch: [46][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1028 (0.1028)	
0.99953055 5.4665935e-07
Epoch: [46][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1098 (0.1203)	
0.9988456 1.3203115e-06
Epoch: [46][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1137 (0.1174)	
0.99956113 5.3334725e-07
loss:  0.09221241622138032 0.0937453528470964
===========>   training    <===========
Epoch: [47][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2246 (0.2246)	
0.99963975 8.622893e-06
===========>   testing    <===========
Epoch: [47][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1110 (0.1110)	
0.99934214 1.00894595e-05
Epoch: [47][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1118 (0.1162)	
0.99874485 4.421382e-06
Epoch: [47][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1400 (0.1150)	
0.99934524 4.4038798e-06
loss:  0.09121009209212916 0.09221241622138032
===========>   training    <===========
Epoch: [48][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.3300 (0.3300)	
0.9991572 0.000111391055
===========>   testing    <===========
Epoch: [48][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0946 (0.0946)	
0.9995437 3.705726e-07
Epoch: [48][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1351 (0.1207)	
0.99918395 2.2663226e-06
Epoch: [48][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1015 (0.1194)	
0.9995721 4.483847e-07
loss:  0.09287028085317295 0.09121009209212916
===========>   training    <===========
Epoch: [49][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.3116 (0.3116)	
0.99960774 4.6659156e-06
===========>   testing    <===========
Epoch: [49][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1127 (0.1127)	
0.99900466 1.1067431e-07
Epoch: [49][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0788 (0.1154)	
0.99726725 1.767774e-06
Epoch: [49][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1235 (0.1136)	
0.9985744 4.777735e-07
loss:  0.08903682591664841 0.09121009209212916
===========>   training    <===========
Epoch: [50][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2794 (0.2794)	
0.9992204 2.1439409e-07
===========>   testing    <===========
Epoch: [50][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0683 (0.0683)	
0.9992611 6.930425e-07
Epoch: [50][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1498 (0.1195)	
0.9978551 1.6508014e-06
Epoch: [50][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0971 (0.1136)	
0.9991823 1.2297905e-06
loss:  0.08826506648257193 0.08903682591664841
===========>   training    <===========
Epoch: [51][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.3094 (0.3094)	
0.9988945 1.3419052e-05
===========>   testing    <===========
Epoch: [51][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1201 (0.1201)	
0.9992681 8.690288e-07
Epoch: [51][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2200 (0.1216)	
0.9989303 2.0436835e-06
Epoch: [51][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1142 (0.1197)	
0.9992225 1.3810352e-06
loss:  0.09271059677267468 0.08826506648257193
===========>   training    <===========
Epoch: [52][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2317 (0.2317)	
0.9991603 7.994425e-06
===========>   testing    <===========
Epoch: [52][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1132 (0.1132)	
0.99956185 2.364344e-06
Epoch: [52][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1111 (0.1111)	
0.9987489 1.7106948e-06
Epoch: [52][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1122 (0.1080)	
0.9994717 9.86803e-07
loss:  0.08595141376923665 0.08826506648257193
===========>   training    <===========
Epoch: [53][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2190 (0.2190)	
0.99950194 9.368074e-06
===========>   testing    <===========
Epoch: [53][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0807 (0.0807)	
0.9993358 2.3190562e-06
Epoch: [53][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0686 (0.1195)	
0.99905664 3.2099288e-06
Epoch: [53][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1668 (0.1152)	
0.9993049 2.6165806e-06
loss:  0.08929853078118855 0.08595141376923665
===========>   training    <===========
Epoch: [54][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.3636 (0.3636)	
0.99926215 3.9670695e-06
===========>   testing    <===========
Epoch: [54][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1171 (0.1171)	
0.99964094 5.6104983e-07
Epoch: [54][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0908 (0.1090)	
0.99938047 5.383926e-07
Epoch: [54][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1255 (0.1075)	
0.99966466 4.1909797e-07
loss:  0.08304938186281174 0.08595141376923665
===========>   training    <===========
Epoch: [55][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1892 (0.1892)	
0.9997279 4.5132615e-06
===========>   testing    <===========
Epoch: [55][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0734 (0.0734)	
0.9995382 1.679837e-06
Epoch: [55][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0757 (0.1109)	
0.9990263 4.5093116e-06
Epoch: [55][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1493 (0.1139)	
0.99956816 1.1927795e-06
loss:  0.08773589302597451 0.08304938186281174
===========>   training    <===========
Epoch: [56][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2383 (0.2383)	
0.9996203 8.267748e-06
===========>   testing    <===========
Epoch: [56][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0814 (0.0814)	
0.999676 1.4303471e-06
Epoch: [56][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0998 (0.1038)	
0.9993789 1.2421765e-06
Epoch: [56][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1408 (0.1030)	
0.9996269 9.659416e-07
loss:  0.08353589398143713 0.08304938186281174
===========>   training    <===========
Epoch: [57][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2945 (0.2945)	
0.9995191 3.5544208e-08
===========>   testing    <===========
Epoch: [57][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0864 (0.0864)	
0.9995517 4.7756666e-06
Epoch: [57][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0996 (0.1071)	
0.9989706 3.3774868e-06
Epoch: [57][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1318 (0.1084)	
0.9996239 4.6925475e-06
loss:  0.09191862248123228 0.08304938186281174
===========>   training    <===========
Epoch: [58][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2059 (0.2059)	
0.99975175 2.8925907e-08
===========>   testing    <===========
Epoch: [58][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0902 (0.0902)	
0.9996828 3.924612e-07
Epoch: [58][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2742 (0.1070)	
0.9977431 8.432545e-07
Epoch: [58][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.1312 (0.1041)	
0.9995859 4.139224e-07
loss:  0.08147997248299532 0.08304938186281174
===========>   training    <===========
Epoch: [59][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2997 (0.2997)	
0.99982196 3.0454694e-07
===========>   testing    <===========
Epoch: [59][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0864 (0.0864)	
0.9997038 3.5632154e-06
Epoch: [59][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.2764 (0.1054)	
0.9994178 8.747514e-06
Epoch: [59][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0009025]	Loss 0.0856 (0.1055)	
0.99973696 5.0806834e-06
loss:  0.08476403044917946 0.08147997248299532
===========>   training    <===========
Epoch: [60][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1975 (0.1975)	
0.99967134 1.4314511e-05
===========>   testing    <===========
Epoch: [60][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0939 (0.0939)	
0.99977845 2.0076646e-05
Epoch: [60][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0988 (0.1075)	
0.99964094 7.0946526e-06
Epoch: [60][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1653 (0.1081)	
0.99978715 8.467588e-06
loss:  0.07955401743527446 0.08147997248299532
===========>   training    <===========
Epoch: [61][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.2276 (0.2276)	
0.9996706 1.3424394e-06
===========>   testing    <===========
Epoch: [61][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0912 (0.0912)	
0.99971837 1.1264721e-06
Epoch: [61][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1157 (0.1153)	
0.99915767 4.741164e-06
Epoch: [61][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0836 (0.1120)	
0.99967194 1.9307163e-06
loss:  0.08477000556535486 0.07955401743527446
===========>   training    <===========
Epoch: [62][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.2903 (0.2903)	
0.99972194 3.4977209e-06
===========>   testing    <===========
Epoch: [62][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0970 (0.0970)	
0.9997507 1.074633e-06
Epoch: [62][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1515 (0.1007)	
0.9994961 2.860285e-06
Epoch: [62][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0905 (0.0959)	
0.99976236 5.0567896e-06
loss:  0.07714000079835137 0.07955401743527446
===========>   training    <===========
Epoch: [63][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1845 (0.1845)	
0.9998472 7.8360006e-07
===========>   testing    <===========
Epoch: [63][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0829 (0.0829)	
0.9997099 5.6075294e-07
Epoch: [63][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0463 (0.1024)	
0.99944645 2.886347e-06
Epoch: [63][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1831 (0.1012)	
0.99972266 1.0243809e-06
loss:  0.0800259603941399 0.07714000079835137
===========>   training    <===========
Epoch: [64][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1932 (0.1932)	
0.99968815 4.623815e-07
===========>   testing    <===========
Epoch: [64][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0841 (0.0841)	
0.9996736 1.0027196e-06
Epoch: [64][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1225 (0.1001)	
0.99955624 3.4247541e-06
Epoch: [64][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1304 (0.0972)	
0.9996898 2.3653952e-06
loss:  0.07596597925695181 0.07714000079835137
===========>   training    <===========
Epoch: [65][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1627 (0.1627)	
0.999824 2.2873085e-06
===========>   testing    <===========
Epoch: [65][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0744 (0.0744)	
0.9996871 1.1363577e-06
Epoch: [65][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1196 (0.0988)	
0.9996375 2.3280259e-06
Epoch: [65][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0834 (0.0986)	
0.9997141 1.3375487e-06
loss:  0.07574204004294294 0.07596597925695181
===========>   training    <===========
Epoch: [66][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.2243 (0.2243)	
0.99983037 4.21013e-05
===========>   testing    <===========
Epoch: [66][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0844 (0.0844)	
0.9997403 6.992019e-07
Epoch: [66][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1338 (0.0991)	
0.9996232 5.550739e-06
Epoch: [66][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0823 (0.0969)	
0.9998072 4.478176e-07
loss:  0.07559101805118118 0.07574204004294294
===========>   training    <===========
Epoch: [67][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1771 (0.1771)	
0.9998677 3.2823475e-06
===========>   testing    <===========
Epoch: [67][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0809 (0.0809)	
0.99974185 2.2514394e-06
Epoch: [67][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.2286 (0.1077)	
0.99966455 6.6708662e-06
Epoch: [67][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1348 (0.1030)	
0.99976224 1.2819863e-06
loss:  0.0819581701911466 0.07559101805118118
===========>   training    <===========
Epoch: [68][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1828 (0.1828)	
0.9998123 1.1234424e-06
===========>   testing    <===========
Epoch: [68][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0815 (0.0815)	
0.9998374 2.4579586e-07
Epoch: [68][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1615 (0.1097)	
0.9994753 1.431858e-06
Epoch: [68][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1360 (0.1055)	
0.99984527 4.7567394e-07
loss:  0.08077883479239867 0.07559101805118118
===========>   training    <===========
Epoch: [69][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.2225 (0.2225)	
0.99986446 1.2445089e-06
===========>   testing    <===========
Epoch: [69][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0735 (0.0735)	
0.99978787 3.694054e-06
Epoch: [69][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1099 (0.0999)	
0.9996352 1.2128255e-05
Epoch: [69][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0657 (0.0966)	
0.9997968 3.4870066e-06
loss:  0.07400883322918006 0.07559101805118118
===========>   training    <===========
Epoch: [70][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1893 (0.1893)	
0.999828 5.467277e-07
===========>   testing    <===========
Epoch: [70][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0790 (0.0790)	
0.999699 8.9200775e-08
Epoch: [70][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0854 (0.1109)	
0.99966204 5.98329e-07
Epoch: [70][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.2318 (0.1135)	
0.99973816 7.46672e-08
loss:  0.08413749517409941 0.07400883322918006
===========>   training    <===========
Epoch: [71][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.2285 (0.2285)	
0.99974114 7.044244e-06
===========>   testing    <===========
Epoch: [71][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0997 (0.0997)	
0.9996582 7.9122725e-07
Epoch: [71][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0928 (0.0947)	
0.9995247 7.7957e-06
Epoch: [71][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1304 (0.0920)	
0.99967027 1.3485959e-06
loss:  0.07061290148072119 0.07400883322918006
===========>   training    <===========
Epoch: [72][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1663 (0.1663)	
0.999542 1.20992825e-08
===========>   testing    <===========
Epoch: [72][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1036 (0.1036)	
0.999838 2.5960293e-08
Epoch: [72][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1020 (0.1074)	
0.99957854 2.9608995e-07
Epoch: [72][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1403 (0.1036)	
0.99982566 1.5475666e-07
loss:  0.0786503443705141 0.07061290148072119
===========>   training    <===========
Epoch: [73][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.2478 (0.2478)	
0.9998436 7.0404562e-06
===========>   testing    <===========
Epoch: [73][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0985 (0.0985)	
0.99985373 2.1039773e-08
Epoch: [73][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0424 (0.1064)	
0.99964714 1.01424355e-07
Epoch: [73][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1797 (0.1015)	
0.9998276 4.1058563e-08
loss:  0.07847534897090458 0.07061290148072119
===========>   training    <===========
Epoch: [74][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.2560 (0.2560)	
0.9999281 9.781344e-07
===========>   testing    <===========
Epoch: [74][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1000 (0.1000)	
0.99962556 5.4675926e-09
Epoch: [74][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0637 (0.1036)	
0.9989889 5.334556e-07
Epoch: [74][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1494 (0.1011)	
0.99964416 2.8703653e-08
loss:  0.08015565514836531 0.07061290148072119
===========>   training    <===========
Epoch: [75][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1875 (0.1875)	
0.99975365 9.201282e-08
===========>   testing    <===========
Epoch: [75][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1161 (0.1161)	
0.9998196 2.1275773e-07
Epoch: [75][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.2017 (0.0960)	
0.9996824 1.21482e-05
Epoch: [75][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0892 (0.0910)	
0.99980766 1.3974783e-06
loss:  0.07143511958482474 0.07061290148072119
===========>   training    <===========
Epoch: [76][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.2519 (0.2519)	
0.99976796 7.3200437e-07
===========>   testing    <===========
Epoch: [76][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1201 (0.1201)	
0.9997228 6.213644e-08
Epoch: [76][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1752 (0.0990)	
0.99954945 9.195104e-07
Epoch: [76][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0727 (0.0982)	
0.9997086 3.6915006e-07
loss:  0.07989305288378235 0.07061290148072119
===========>   training    <===========
Epoch: [77][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1916 (0.1916)	
0.99973136 7.49554e-09
===========>   testing    <===========
Epoch: [77][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1049 (0.1049)	
0.99977714 3.278498e-08
Epoch: [77][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0514 (0.0904)	
0.99951863 1.905559e-07
Epoch: [77][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0526 (0.0889)	
0.9997247 3.5196774e-07
loss:  0.07035651935501697 0.07061290148072119
===========>   training    <===========
Epoch: [78][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1717 (0.1717)	
0.99986863 2.4331462e-06
===========>   testing    <===========
Epoch: [78][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0979 (0.0979)	
0.9997911 1.3974314e-07
Epoch: [78][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1184 (0.0884)	
0.9996667 3.1981524e-06
Epoch: [78][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0742 (0.0887)	
0.9997781 1.0156641e-06
loss:  0.07277190148559742 0.07035651935501697
===========>   training    <===========
Epoch: [79][0/46]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.2315 (0.2315)	
0.9997801 6.334584e-10
===========>   testing    <===========
Epoch: [79][0/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.0868 (0.0868)	
0.99976 3.267891e-07
Epoch: [79][100/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1067 (0.0912)	
0.9995763 8.9371554e-07
Epoch: [79][200/289]	Lr-deconv: [0.0]	Lr-other: [0.000857375]	Loss 0.1326 (0.0905)	
0.99977297 4.05808e-07
loss:  0.07196721668654238 0.07035651935501697
===========>   training    <===========
Epoch: [80][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.2112 (0.2112)	
0.9997968 2.4258666e-06
===========>   testing    <===========
Epoch: [80][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0712 (0.0712)	
0.9997224 7.51183e-08
Epoch: [80][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0545 (0.0985)	
0.999466 1.558353e-06
Epoch: [80][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1209 (0.0930)	
0.99975187 1.5801726e-06
loss:  0.07228585507243701 0.07035651935501697
===========>   training    <===========
Epoch: [81][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.2107 (0.2107)	
0.9995938 6.706275e-05
===========>   testing    <===========
Epoch: [81][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0933 (0.0933)	
0.9997832 8.242e-08
Epoch: [81][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0454 (0.0934)	
0.9997079 2.5980088e-07
Epoch: [81][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1240 (0.0923)	
0.9997861 1.05201664e-07
loss:  0.07645959733951868 0.07035651935501697
===========>   training    <===========
Epoch: [82][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1700 (0.1700)	
0.9998927 9.698233e-06
===========>   testing    <===========
Epoch: [82][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1108 (0.1108)	
0.9998785 1.665079e-07
Epoch: [82][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0778 (0.0902)	
0.9997696 6.2809016e-07
Epoch: [82][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0745 (0.0873)	
0.9998758 1.9121418e-07
loss:  0.07098785734338753 0.07035651935501697
===========>   training    <===========
Epoch: [83][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.2518 (0.2518)	
0.99982077 1.7621925e-06
===========>   testing    <===========
Epoch: [83][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1247 (0.1247)	
0.99988496 1.059202e-06
Epoch: [83][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0461 (0.0979)	
0.99975926 1.039169e-06
Epoch: [83][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1281 (0.0957)	
0.9998987 8.4711775e-07
loss:  0.07583901736850795 0.07035651935501697
===========>   training    <===========
Epoch: [84][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1739 (0.1739)	
0.99992347 6.335121e-07
===========>   testing    <===========
Epoch: [84][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1017 (0.1017)	
0.9998951 2.7276383e-07
Epoch: [84][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1405 (0.0981)	
0.99985075 1.2626285e-06
Epoch: [84][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1218 (0.0961)	
0.9998975 1.4250588e-06
loss:  0.07232422753563161 0.07035651935501697
===========>   training    <===========
Epoch: [85][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1350 (0.1350)	
0.9999161 2.1929557e-06
===========>   testing    <===========
Epoch: [85][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0977 (0.0977)	
0.9998468 2.4258885e-08
Epoch: [85][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0521 (0.0894)	
0.9997428 2.1079745e-07
Epoch: [85][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1085 (0.0879)	
0.99986446 8.373868e-08
loss:  0.06919815315605204 0.07035651935501697
===========>   training    <===========
Epoch: [86][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.2117 (0.2117)	
0.99990594 8.419993e-07
===========>   testing    <===========
Epoch: [86][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1036 (0.1036)	
0.99987125 6.387486e-09
Epoch: [86][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0865 (0.0931)	
0.99976283 2.6919826e-08
Epoch: [86][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0960 (0.0895)	
0.99986637 9.727573e-09
loss:  0.07225174704404125 0.06919815315605204
===========>   training    <===========
Epoch: [87][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1877 (0.1877)	
0.9998623 2.5682922e-07
===========>   testing    <===========
Epoch: [87][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0872 (0.0872)	
0.9997693 2.1169859e-07
Epoch: [87][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0484 (0.0859)	
0.99952984 1.0096998e-06
Epoch: [87][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0819 (0.0842)	
0.99986696 8.3401704e-07
loss:  0.06720942455357826 0.06919815315605204
===========>   training    <===========
Epoch: [88][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1649 (0.1649)	
0.9999019 3.795613e-07
===========>   testing    <===========
Epoch: [88][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0950 (0.0950)	
0.9997428 3.7624456e-08
Epoch: [88][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0403 (0.0921)	
0.99967384 7.985709e-08
Epoch: [88][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1262 (0.0885)	
0.999806 1.6578095e-07
loss:  0.06979503130553932 0.06720942455357826
===========>   training    <===========
Epoch: [89][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1585 (0.1585)	
0.99977916 5.2420933e-08
===========>   testing    <===========
Epoch: [89][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0738 (0.0738)	
0.9998683 3.521181e-08
Epoch: [89][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0545 (0.0967)	
0.99938357 2.4354603e-07
Epoch: [89][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0529 (0.0936)	
0.999881 4.2590696e-07
loss:  0.07450660876447657 0.06720942455357826
===========>   training    <===========
Epoch: [90][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1871 (0.1871)	
0.99992394 5.0058776e-07
===========>   testing    <===========
Epoch: [90][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0937 (0.0937)	
0.9996824 2.8045525e-08
Epoch: [90][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0620 (0.0906)	
0.99953234 4.587207e-08
Epoch: [90][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0953 (0.0917)	
0.9997421 4.0209315e-08
loss:  0.07081013324757235 0.06720942455357826
===========>   training    <===========
Epoch: [91][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.2381 (0.2381)	
0.99970335 1.4289686e-06
===========>   testing    <===========
Epoch: [91][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0893 (0.0893)	
0.99979895 9.538498e-07
Epoch: [91][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0484 (0.0945)	
0.99973434 2.0801413e-06
Epoch: [91][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0969 (0.0917)	
0.9997931 1.3582636e-06
loss:  0.0724436189691745 0.06720942455357826
===========>   training    <===========
Epoch: [92][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.3174 (0.3174)	
0.9998141 1.696895e-06
===========>   testing    <===========
Epoch: [92][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0941 (0.0941)	
0.9996556 1.0040232e-07
Epoch: [92][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0804 (0.0970)	
0.9991534 7.2119474e-07
Epoch: [92][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1399 (0.0953)	
0.9997329 2.5257873e-07
loss:  0.07277258934138853 0.06720942455357826
===========>   training    <===========
Epoch: [93][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.2056 (0.2056)	
0.9997749 1.9065715e-07
===========>   testing    <===========
Epoch: [93][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0945 (0.0945)	
0.99984574 4.5145387e-07
Epoch: [93][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1093 (0.0904)	
0.9998227 9.36626e-06
Epoch: [93][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0795 (0.0855)	
0.9999132 1.0153077e-06
loss:  0.06614981589948388 0.06720942455357826
===========>   training    <===========
Epoch: [94][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.2494 (0.2494)	
0.999899 3.4356838e-06
===========>   testing    <===========
Epoch: [94][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0597 (0.0597)	
0.9998473 3.9503303e-07
Epoch: [94][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1523 (0.0922)	
0.9997844 2.5478953e-06
Epoch: [94][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1365 (0.0910)	
0.9998683 1.055308e-06
loss:  0.07053559680003174 0.06614981589948388
===========>   training    <===========
Epoch: [95][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1982 (0.1982)	
0.9998293 5.053769e-07
===========>   testing    <===========
Epoch: [95][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0679 (0.0679)	
0.9998184 3.6668516e-07
Epoch: [95][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.4022 (0.0913)	
0.999671 5.9896786e-07
Epoch: [95][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0829 (0.0870)	
0.999845 4.1497967e-07
loss:  0.06872505201156687 0.06614981589948388
===========>   training    <===========
Epoch: [96][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.2281 (0.2281)	
0.9998568 4.970172e-07
===========>   testing    <===========
Epoch: [96][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0997 (0.0997)	
0.99990046 1.3818152e-06
Epoch: [96][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1376 (0.0995)	
0.99980384 3.1486782e-06
Epoch: [96][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1115 (0.0946)	
0.99989533 8.859795e-07
loss:  0.07146554391141535 0.06614981589948388
===========>   training    <===========
Epoch: [97][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.2115 (0.2115)	
0.999795 1.1415929e-09
===========>   testing    <===========
Epoch: [97][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0970 (0.0970)	
0.9998777 7.772857e-08
Epoch: [97][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1542 (0.0878)	
0.999676 1.2146543e-06
Epoch: [97][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1220 (0.0850)	
0.99989665 9.0122015e-08
loss:  0.06674199950073256 0.06614981589948388
===========>   training    <===========
Epoch: [98][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1756 (0.1756)	
0.9999181 4.1308518e-05
===========>   testing    <===========
Epoch: [98][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1124 (0.1124)	
0.9997812 1.1982665e-07
Epoch: [98][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.2417 (0.1036)	
0.9993088 7.833625e-07
Epoch: [98][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1191 (0.0995)	
0.99985015 4.3786883e-07
loss:  0.07773774391362398 0.06614981589948388
===========>   training    <===========
Epoch: [99][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.1686 (0.1686)	
0.9997842 1.9482527e-06
===========>   testing    <===========
Epoch: [99][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0822 (0.0822)	
0.9999143 1.3951785e-06
Epoch: [99][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0534 (0.0833)	
0.99983764 2.3217601e-06
Epoch: [99][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0008145062499999999]	Loss 0.0736 (0.0805)	
0.99992347 8.153554e-07
loss:  0.06309515507346064 0.06614981589948388
===========>   training    <===========
Epoch: [100][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.2554 (0.2554)	
0.9999052 2.559861e-07
===========>   testing    <===========
Epoch: [100][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0811 (0.0811)	
0.99983704 1.6186853e-07
Epoch: [100][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0960 (0.0806)	
0.9997392 7.455888e-07
Epoch: [100][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1155 (0.0800)	
0.9998884 4.6889187e-07
loss:  0.06244320712472207 0.06309515507346064
===========>   training    <===========
Epoch: [101][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.2985 (0.2985)	
0.9997812 1.9563292e-06
===========>   testing    <===========
Epoch: [101][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1116 (0.1116)	
0.99979407 5.5605238e-08
Epoch: [101][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0714 (0.0873)	
0.99894625 1.252978e-07
Epoch: [101][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1804 (0.0834)	
0.9998367 2.828192e-07
loss:  0.06532999938213002 0.06244320712472207
===========>   training    <===========
Epoch: [102][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1749 (0.1749)	
0.999928 2.0691633e-07
===========>   testing    <===========
Epoch: [102][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1227 (0.1227)	
0.999884 2.0747962e-06
Epoch: [102][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0926 (0.0852)	
0.9996561 1.647549e-05
Epoch: [102][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1157 (0.0814)	
0.99989486 1.5333358e-06
loss:  0.06579736730714791 0.06244320712472207
===========>   training    <===========
Epoch: [103][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1622 (0.1622)	
0.9998184 1.2420912e-06
===========>   testing    <===========
Epoch: [103][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0942 (0.0942)	
0.9998623 6.1110603e-07
Epoch: [103][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.2434 (0.0964)	
0.99982625 9.267513e-07
Epoch: [103][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1134 (0.0946)	
0.9998969 9.793207e-07
loss:  0.07094745193502616 0.06244320712472207
===========>   training    <===========
Epoch: [104][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1753 (0.1753)	
0.9999403 6.509349e-06
===========>   testing    <===========
Epoch: [104][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0900 (0.0900)	
0.99987066 2.892752e-07
Epoch: [104][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1247 (0.0877)	
0.99980956 5.3598255e-06
Epoch: [104][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1206 (0.0823)	
0.9998857 1.345364e-06
loss:  0.0640804728152351 0.06244320712472207
===========>   training    <===========
Epoch: [105][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1683 (0.1683)	
0.999863 5.216729e-07
===========>   testing    <===========
Epoch: [105][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0923 (0.0923)	
0.99992466 1.4277836e-06
Epoch: [105][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1597 (0.0820)	
0.9997907 7.2891394e-06
Epoch: [105][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0999 (0.0795)	
0.99992216 1.2046318e-06
loss:  0.06279621975964689 0.06244320712472207
===========>   training    <===========
Epoch: [106][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.2376 (0.2376)	
0.999933 6.31052e-07
===========>   testing    <===========
Epoch: [106][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0687 (0.0687)	
0.9998754 7.057708e-08
Epoch: [106][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.2895 (0.0862)	
0.99946016 2.665553e-07
Epoch: [106][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0809 (0.0819)	
0.9998747 8.658953e-08
loss:  0.0635343408962662 0.06244320712472207
===========>   training    <===========
Epoch: [107][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1370 (0.1370)	
0.99990284 3.250662e-07
===========>   testing    <===========
Epoch: [107][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0855 (0.0855)	
0.9999021 5.8239226e-07
Epoch: [107][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0881 (0.0946)	
0.9995851 6.132135e-07
Epoch: [107][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1810 (0.0934)	
0.9999069 1.453821e-07
loss:  0.0713590490032181 0.06244320712472207
===========>   training    <===========
Epoch: [108][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.2050 (0.2050)	
0.9997546 2.7942533e-06
===========>   testing    <===========
Epoch: [108][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0921 (0.0921)	
0.99993014 6.516589e-07
Epoch: [108][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.2324 (0.0906)	
0.9994886 8.712884e-07
Epoch: [108][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1938 (0.0869)	
0.9999274 7.892768e-07
loss:  0.06810534814082403 0.06244320712472207
===========>   training    <===========
Epoch: [109][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1536 (0.1536)	
0.9998927 1.0987397e-05
===========>   testing    <===========
Epoch: [109][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0936 (0.0936)	
0.9998752 5.2093906e-08
Epoch: [109][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1000 (0.0876)	
0.9991725 1.0174057e-07
Epoch: [109][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1493 (0.0838)	
0.9998605 4.848449e-08
loss:  0.06459513155161156 0.06244320712472207
===========>   training    <===========
Epoch: [110][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1989 (0.1989)	
0.9999118 2.0568825e-07
===========>   testing    <===========
Epoch: [110][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0621 (0.0621)	
0.99984515 4.597863e-07
Epoch: [110][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.7225 (0.0869)	
0.9996346 4.5721652e-07
Epoch: [110][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0752 (0.0806)	
0.9998934 2.7264915e-07
loss:  0.06113791708755967 0.06244320712472207
===========>   training    <===========
Epoch: [111][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1795 (0.1795)	
0.99986637 6.390352e-08
===========>   testing    <===========
Epoch: [111][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0966 (0.0966)	
0.9998067 9.744269e-07
Epoch: [111][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.3988 (0.0838)	
0.99947304 7.9547095e-07
Epoch: [111][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1050 (0.0796)	
0.99983215 3.297208e-07
loss:  0.06149906380446435 0.06113791708755967
===========>   training    <===========
Epoch: [112][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1485 (0.1485)	
0.99988115 4.82475e-06
===========>   testing    <===========
Epoch: [112][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0931 (0.0931)	
0.99988794 1.4247541e-07
Epoch: [112][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.2197 (0.0927)	
0.9997819 3.0214176e-07
Epoch: [112][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1358 (0.0893)	
0.9998909 2.3953274e-07
loss:  0.06845590730295925 0.06113791708755967
===========>   training    <===========
Epoch: [113][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1743 (0.1743)	
0.99992824 8.980275e-07
===========>   testing    <===========
Epoch: [113][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0993 (0.0993)	
0.9997408 3.1353122e-07
Epoch: [113][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.4040 (0.0884)	
0.99944013 1.3171972e-07
Epoch: [113][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0955 (0.0831)	
0.99980825 2.3019432e-07
loss:  0.06482914255623495 0.06113791708755967
===========>   training    <===========
Epoch: [114][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.3097 (0.3097)	
0.99965334 2.8683844e-06
===========>   testing    <===========
Epoch: [114][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0680 (0.0680)	
0.999838 9.716118e-08
Epoch: [114][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.2831 (0.0858)	
0.99974173 2.9786005e-07
Epoch: [114][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0814 (0.0793)	
0.99987996 8.242472e-08
loss:  0.06196839336959992 0.06113791708755967
===========>   training    <===========
Epoch: [115][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1524 (0.1524)	
0.9997062 4.2827256e-07
===========>   testing    <===========
Epoch: [115][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0607 (0.0607)	
0.99984705 9.862181e-08
Epoch: [115][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1178 (0.0844)	
0.9997439 5.838515e-07
Epoch: [115][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1212 (0.0798)	
0.99992645 1.0692763e-08
loss:  0.06286443560312438 0.06113791708755967
===========>   training    <===========
Epoch: [116][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1926 (0.1926)	
0.99978024 8.4026674e-08
===========>   testing    <===========
Epoch: [116][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1070 (0.1070)	
0.99951804 1.1926864e-08
Epoch: [116][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1200 (0.0771)	
0.9995247 7.601679e-09
Epoch: [116][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0878 (0.0740)	
0.99979395 7.059576e-09
loss:  0.05947886560036186 0.06113791708755967
===========>   training    <===========
Epoch: [117][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1644 (0.1644)	
0.9996481 6.957384e-07
===========>   testing    <===========
Epoch: [117][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1868 (0.1868)	
0.99980325 1.6684999e-07
Epoch: [117][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.2515 (0.0858)	
0.99980766 7.94411e-09
Epoch: [117][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1178 (0.0807)	
0.9999049 3.2222193e-08
loss:  0.06458310657816757 0.05947886560036186
===========>   training    <===========
Epoch: [118][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1930 (0.1930)	
0.9998124 9.86611e-07
===========>   testing    <===========
Epoch: [118][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1016 (0.1016)	
0.99981683 3.1174437e-07
Epoch: [118][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1571 (0.0820)	
0.9996618 6.199072e-08
Epoch: [118][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1063 (0.0785)	
0.9998337 8.934959e-08
loss:  0.06085877107869975 0.05947886560036186
===========>   training    <===========
Epoch: [119][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1672 (0.1672)	
0.999912 3.8241897e-06
===========>   testing    <===========
Epoch: [119][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.0982 (0.0982)	
0.9997689 4.8548895e-07
Epoch: [119][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1588 (0.0815)	
0.99949014 3.335804e-07
Epoch: [119][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007737809374999998]	Loss 0.1395 (0.0776)	
0.9998211 3.9163037e-08
loss:  0.060386222118051114 0.05947886560036186
===========>   training    <===========
Epoch: [120][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1513 (0.1513)	
0.99986446 2.4348441e-08
===========>   testing    <===========
Epoch: [120][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1032 (0.1032)	
0.9998429 7.254005e-08
Epoch: [120][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1675 (0.0785)	
0.9997304 1.0109988e-07
Epoch: [120][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1028 (0.0742)	
0.9998072 6.0207476e-08
loss:  0.05956706514016186 0.05947886560036186
===========>   training    <===========
Epoch: [121][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1761 (0.1761)	
0.9999232 5.8321046e-08
===========>   testing    <===========
Epoch: [121][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0675 (0.0675)	
0.9999175 8.8376436e-07
Epoch: [121][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1144 (0.0962)	
0.99979025 6.0582204e-07
Epoch: [121][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1558 (0.0925)	
0.9999263 1.0708952e-07
loss:  0.07023683684129811 0.05947886560036186
===========>   training    <===========
Epoch: [122][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1730 (0.1730)	
0.9999374 1.0169225e-07
===========>   testing    <===========
Epoch: [122][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1182 (0.1182)	
0.99987423 9.5354056e-07
Epoch: [122][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0354 (0.0886)	
0.9997354 1.0455389e-06
Epoch: [122][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.2016 (0.0863)	
0.9998859 3.361749e-07
loss:  0.06560009742444461 0.05947886560036186
===========>   training    <===========
Epoch: [123][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1793 (0.1793)	
0.99992526 4.0989735e-06
===========>   testing    <===========
Epoch: [123][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1066 (0.1066)	
0.99992454 4.771251e-07
Epoch: [123][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.3292 (0.0935)	
0.99986565 7.621382e-07
Epoch: [123][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1253 (0.0872)	
0.99991965 3.5088928e-07
loss:  0.06709735631675529 0.05947886560036186
===========>   training    <===========
Epoch: [124][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1702 (0.1702)	
0.9999237 4.505469e-06
===========>   testing    <===========
Epoch: [124][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0724 (0.0724)	
0.9998504 1.3536325e-07
Epoch: [124][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0438 (0.0861)	
0.9995962 8.866983e-08
Epoch: [124][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.2054 (0.0828)	
0.99987245 5.592678e-08
loss:  0.06507447535985078 0.05947886560036186
===========>   training    <===========
Epoch: [125][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1428 (0.1428)	
0.9997863 8.7018987e-07
===========>   testing    <===========
Epoch: [125][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0752 (0.0752)	
0.99974006 3.962529e-07
Epoch: [125][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1149 (0.0835)	
0.9997882 4.6420496e-07
Epoch: [125][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1536 (0.0762)	
0.99979657 3.8788593e-07
loss:  0.05870130019638631 0.05947886560036186
===========>   training    <===========
Epoch: [126][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1871 (0.1871)	
0.9997749 2.4794323e-07
===========>   testing    <===========
Epoch: [126][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0621 (0.0621)	
0.9999262 5.3515237e-07
Epoch: [126][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.2653 (0.0822)	
0.99974865 2.5090898e-07
Epoch: [126][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0895 (0.0774)	
0.9999398 4.7790746e-07
loss:  0.06012211784047361 0.05870130019638631
===========>   training    <===========
Epoch: [127][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1641 (0.1641)	
0.9999124 4.446186e-06
===========>   testing    <===========
Epoch: [127][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0573 (0.0573)	
0.99990094 7.5607026e-08
Epoch: [127][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.2080 (0.0812)	
0.9998995 1.8961832e-07
Epoch: [127][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0745 (0.0758)	
0.9999014 2.3845013e-07
loss:  0.0587459738301257 0.05870130019638631
===========>   training    <===========
Epoch: [128][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1472 (0.1472)	
0.99988246 4.882488e-07
===========>   testing    <===========
Epoch: [128][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0810 (0.0810)	
0.9997503 1.8203023e-08
Epoch: [128][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1621 (0.0822)	
0.99938726 5.607878e-08
Epoch: [128][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0868 (0.0767)	
0.9998578 8.277143e-09
loss:  0.05979760817713242 0.05870130019638631
===========>   training    <===========
Epoch: [129][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1670 (0.1670)	
0.9998939 2.4486063e-08
===========>   testing    <===========
Epoch: [129][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0664 (0.0664)	
0.99988973 1.5006985e-07
Epoch: [129][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1386 (0.0844)	
0.999895 2.2382399e-07
Epoch: [129][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0590 (0.0779)	
0.9999275 4.6815106e-08
loss:  0.059636847557561046 0.05870130019638631
===========>   training    <===========
Epoch: [130][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1686 (0.1686)	
0.99991333 1.9535628e-06
===========>   testing    <===========
Epoch: [130][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0831 (0.0831)	
0.99993026 4.1948383e-08
Epoch: [130][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.3364 (0.0822)	
0.99985576 6.1490475e-08
Epoch: [130][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1019 (0.0771)	
0.9998685 1.8929702e-08
loss:  0.058648604926775705 0.05870130019638631
===========>   training    <===========
Epoch: [131][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1322 (0.1322)	
0.9999237 5.5822763e-08
===========>   testing    <===========
Epoch: [131][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0665 (0.0665)	
0.99988604 3.8718892e-07
Epoch: [131][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1084 (0.0802)	
0.9996623 2.6864518e-07
Epoch: [131][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0991 (0.0767)	
0.99987936 3.2510246e-07
loss:  0.05902971375452215 0.058648604926775705
===========>   training    <===========
Epoch: [132][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1320 (0.1320)	
0.9999107 1.030941e-07
===========>   testing    <===========
Epoch: [132][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1117 (0.1117)	
0.9999275 1.3418898e-07
Epoch: [132][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.3228 (0.0826)	
0.9998934 2.9327182e-07
Epoch: [132][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0709 (0.0790)	
0.99992216 1.6208973e-07
loss:  0.05889093379422872 0.058648604926775705
===========>   training    <===========
Epoch: [133][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1674 (0.1674)	
0.9999496 5.5614987e-07
===========>   testing    <===========
Epoch: [133][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0547 (0.0547)	
0.99980944 3.3060015e-08
Epoch: [133][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0483 (0.0906)	
0.9998555 1.7814354e-07
Epoch: [133][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1082 (0.0852)	
0.9999304 3.329974e-08
loss:  0.06217294689939423 0.058648604926775705
===========>   training    <===========
Epoch: [134][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1879 (0.1879)	
0.9998976 4.952744e-06
===========>   testing    <===========
Epoch: [134][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0905 (0.0905)	
0.9998022 9.95687e-09
Epoch: [134][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1665 (0.0805)	
0.9995297 3.614377e-08
Epoch: [134][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0732 (0.0756)	
0.99988806 7.615538e-09
loss:  0.058423934683758505 0.058648604926775705
===========>   training    <===========
Epoch: [135][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1539 (0.1539)	
0.99997663 3.9031715e-08
===========>   testing    <===========
Epoch: [135][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0530 (0.0530)	
0.9999391 2.3759972e-08
Epoch: [135][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1518 (0.0811)	
0.999881 1.2356706e-07
Epoch: [135][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1237 (0.0784)	
0.99992895 6.7578756e-09
loss:  0.05800687338241406 0.058423934683758505
===========>   training    <===========
Epoch: [136][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1614 (0.1614)	
0.99992836 8.928669e-09
===========>   testing    <===========
Epoch: [136][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0766 (0.0766)	
0.99995387 7.7455375e-08
Epoch: [136][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.2910 (0.0825)	
0.9998424 3.2099945e-07
Epoch: [136][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0525 (0.0742)	
0.9999554 7.846397e-08
loss:  0.057932701199672865 0.05800687338241406
===========>   training    <===========
Epoch: [137][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1594 (0.1594)	
0.9999248 2.1341127e-07
===========>   testing    <===========
Epoch: [137][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0697 (0.0697)	
0.99987316 4.189687e-09
Epoch: [137][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1202 (0.0933)	
0.9997793 3.6873658e-08
Epoch: [137][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.2303 (0.0896)	
0.9998222 1.4780356e-09
loss:  0.06933282298836418 0.057932701199672865
===========>   training    <===========
Epoch: [138][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1447 (0.1447)	
0.99989796 4.9051422e-08
===========>   testing    <===========
Epoch: [138][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0622 (0.0622)	
0.999869 4.1652353e-08
Epoch: [138][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0786 (0.0794)	
0.99962175 1.18196155e-07
Epoch: [138][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1015 (0.0754)	
0.9998429 2.2916257e-08
loss:  0.05863974908795533 0.057932701199672865
===========>   training    <===========
Epoch: [139][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.1899 (0.1899)	
0.9998841 5.307711e-07
===========>   testing    <===========
Epoch: [139][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0650 (0.0650)	
0.99985623 4.623511e-08
Epoch: [139][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0616 (0.0813)	
0.9997956 6.672628e-08
Epoch: [139][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0007350918906249999]	Loss 0.0519 (0.0746)	
0.99986327 3.2647943e-08
loss:  0.057342788060091965 0.057932701199672865
===========>   training    <===========
Epoch: [140][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1817 (0.1817)	
0.99994135 3.8901202e-08
===========>   testing    <===========
Epoch: [140][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0445 (0.0445)	
0.9999534 5.9576458e-08
Epoch: [140][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1841 (0.0767)	
0.9999355 8.3423835e-08
Epoch: [140][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1073 (0.0705)	
0.99995136 7.006458e-08
loss:  0.05748437321109767 0.057342788060091965
===========>   training    <===========
Epoch: [141][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1424 (0.1424)	
0.9999541 1.3669049e-06
===========>   testing    <===========
Epoch: [141][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0844 (0.0844)	
0.99995136 1.0517076e-07
Epoch: [141][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1549 (0.0792)	
0.99987435 5.1616297e-08
Epoch: [141][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0957 (0.0742)	
0.9999486 3.001366e-08
loss:  0.05869846349814489 0.057342788060091965
===========>   training    <===========
Epoch: [142][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1624 (0.1624)	
0.99993277 1.2758717e-07
===========>   testing    <===========
Epoch: [142][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0580 (0.0580)	
0.9997087 2.7899125e-08
Epoch: [142][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.2378 (0.0844)	
0.99976987 4.4622233e-08
Epoch: [142][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1600 (0.0768)	
0.99987364 2.2122453e-08
loss:  0.05743237057052619 0.057342788060091965
===========>   training    <===========
Epoch: [143][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1519 (0.1519)	
0.9999094 3.417934e-07
===========>   testing    <===========
Epoch: [143][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0524 (0.0524)	
0.99991655 5.209828e-08
Epoch: [143][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1582 (0.0770)	
0.9997018 1.4377868e-07
Epoch: [143][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1028 (0.0709)	
0.9999069 2.6371923e-08
loss:  0.05443959697671685 0.057342788060091965
===========>   training    <===========
Epoch: [144][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1154 (0.1154)	
0.9999275 1.8664078e-06
===========>   testing    <===========
Epoch: [144][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0482 (0.0482)	
0.99994004 5.350962e-08
Epoch: [144][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.2579 (0.0768)	
0.9998149 1.2361e-06
Epoch: [144][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0823 (0.0714)	
0.9999374 1.037399e-07
loss:  0.057102008478502 0.05443959697671685
===========>   training    <===========
Epoch: [145][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.2100 (0.2100)	
0.99995935 1.6734891e-06
===========>   testing    <===========
Epoch: [145][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1012 (0.1012)	
0.9998554 2.5675762e-08
Epoch: [145][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0681 (0.0737)	
0.9996959 2.7356064e-08
Epoch: [145][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0638 (0.0690)	
0.99991 2.1019678e-08
loss:  0.05413261101590083 0.05443959697671685
===========>   training    <===========
Epoch: [146][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1716 (0.1716)	
0.9999447 2.4553128e-08
===========>   testing    <===========
Epoch: [146][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0650 (0.0650)	
0.9999244 1.1994705e-07
Epoch: [146][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0466 (0.0815)	
0.9997098 2.1429332e-07
Epoch: [146][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0618 (0.0756)	
0.9999306 3.4976193e-08
loss:  0.05828148797384869 0.05413261101590083
===========>   training    <===========
Epoch: [147][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1867 (0.1867)	
0.99977094 1.2684008e-07
===========>   testing    <===========
Epoch: [147][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0718 (0.0718)	
0.9999559 1.1151914e-07
Epoch: [147][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0650 (0.0738)	
0.99985695 2.6020555e-07
Epoch: [147][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0676 (0.0686)	
0.99996126 2.0724407e-08
loss:  0.05337699209870872 0.05413261101590083
===========>   training    <===========
Epoch: [148][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1397 (0.1397)	
0.9998678 2.9408878e-07
===========>   testing    <===========
Epoch: [148][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0855 (0.0855)	
0.99993646 3.3753602e-07
Epoch: [148][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0768 (0.0758)	
0.9998055 1.6298958e-07
Epoch: [148][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1428 (0.0699)	
0.99994016 3.0952812e-08
loss:  0.05468517458697231 0.05337699209870872
===========>   training    <===========
Epoch: [149][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1306 (0.1306)	
0.99996006 2.322769e-07
===========>   testing    <===========
Epoch: [149][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0674 (0.0674)	
0.99991393 6.6913884e-08
Epoch: [149][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0549 (0.0754)	
0.9998067 3.7746798e-08
Epoch: [149][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0927 (0.0713)	
0.99992955 2.9208213e-08
loss:  0.05481343436075636 0.05337699209870872
===========>   training    <===========
Epoch: [150][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1856 (0.1856)	
0.99993145 4.1397882e-07
===========>   testing    <===========
Epoch: [150][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0583 (0.0583)	
0.9998591 1.904721e-06
Epoch: [150][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0312 (0.0784)	
0.9997534 2.0968196e-06
Epoch: [150][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1333 (0.0736)	
0.9998952 1.0144965e-06
loss:  0.05645378535806023 0.05337699209870872
===========>   training    <===========
Epoch: [151][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1209 (0.1209)	
0.9999385 1.16274e-06
===========>   testing    <===========
Epoch: [151][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0571 (0.0571)	
0.99962556 5.7213743e-08
Epoch: [151][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0391 (0.0803)	
0.9997719 6.7938586e-08
Epoch: [151][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1590 (0.0759)	
0.99977106 2.2828353e-08
loss:  0.0574697036321824 0.05337699209870872
===========>   training    <===========
Epoch: [152][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1757 (0.1757)	
0.9998841 7.931333e-07
===========>   testing    <===========
Epoch: [152][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0714 (0.0714)	
0.9998512 5.8290345e-07
Epoch: [152][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0348 (0.0861)	
0.99978155 1.1724833e-06
Epoch: [152][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1303 (0.0800)	
0.99986684 2.450831e-07
loss:  0.059786229818756476 0.05337699209870872
===========>   training    <===========
Epoch: [153][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1503 (0.1503)	
0.99982494 6.229906e-06
===========>   testing    <===========
Epoch: [153][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0498 (0.0498)	
0.9998666 2.3938288e-08
Epoch: [153][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0338 (0.0780)	
0.9996258 2.669377e-09
Epoch: [153][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1317 (0.0768)	
0.9998902 7.004932e-09
loss:  0.057066926264593754 0.05337699209870872
===========>   training    <===========
Epoch: [154][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.2494 (0.2494)	
0.99992883 2.7555952e-08
===========>   testing    <===========
Epoch: [154][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0488 (0.0488)	
0.99989057 7.301381e-08
Epoch: [154][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0801 (0.0764)	
0.9997869 5.595025e-08
Epoch: [154][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1271 (0.0715)	
0.9998753 3.3778104e-08
loss:  0.05375384825730678 0.05337699209870872
===========>   training    <===========
Epoch: [155][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1348 (0.1348)	
0.999951 6.4508145e-06
===========>   testing    <===========
Epoch: [155][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0697 (0.0697)	
0.9999567 5.4088353e-08
Epoch: [155][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0857 (0.0769)	
0.99990475 2.423405e-08
Epoch: [155][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1199 (0.0713)	
0.99996066 1.8783322e-08
loss:  0.05445230507500265 0.05337699209870872
===========>   training    <===========
Epoch: [156][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1504 (0.1504)	
0.99995935 5.833567e-07
===========>   testing    <===========
Epoch: [156][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0652 (0.0652)	
0.99997807 2.2166691e-06
Epoch: [156][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1261 (0.0777)	
0.99991095 3.3035565e-07
Epoch: [156][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1318 (0.0723)	
0.9999727 2.0939392e-07
loss:  0.05514599486865834 0.05337699209870872
===========>   training    <===========
Epoch: [157][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1934 (0.1934)	
0.99996996 4.780346e-07
===========>   testing    <===========
Epoch: [157][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0962 (0.0962)	
0.9999076 2.4267166e-08
Epoch: [157][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0680 (0.0795)	
0.9996488 7.282323e-09
Epoch: [157][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1367 (0.0758)	
0.9999168 3.9160404e-09
loss:  0.057385397820976736 0.05337699209870872
===========>   training    <===========
Epoch: [158][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1459 (0.1459)	
0.99995697 4.782494e-08
===========>   testing    <===========
Epoch: [158][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1203 (0.1203)	
0.999869 1.8824626e-07
Epoch: [158][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.4111 (0.0812)	
0.9997948 1.7692486e-07
Epoch: [158][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0823 (0.0727)	
0.9998925 3.9730704e-08
loss:  0.05668666473674633 0.05337699209870872
===========>   training    <===========
Epoch: [159][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1757 (0.1757)	
0.9999362 6.446138e-08
===========>   testing    <===========
Epoch: [159][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0650 (0.0650)	
0.999912 8.3648175e-08
Epoch: [159][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.0750 (0.0756)	
0.9998118 1.6497911e-07
Epoch: [159][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006983372960937497]	Loss 0.1048 (0.0713)	
0.9999372 2.489112e-08
loss:  0.05530279593446341 0.05337699209870872
===========>   training    <===========
Epoch: [160][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1255 (0.1255)	
0.9998739 5.2861178e-09
===========>   testing    <===========
Epoch: [160][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0763 (0.0763)	
0.99994314 5.2171094e-06
Epoch: [160][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0587 (0.0741)	
0.99991894 5.6618906e-06
Epoch: [160][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1127 (0.0712)	
0.9999473 2.6053358e-06
loss:  0.05407342699251094 0.05337699209870872
===========>   training    <===========
Epoch: [161][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1279 (0.1279)	
0.99996173 9.922868e-07
===========>   testing    <===========
Epoch: [161][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0865 (0.0865)	
0.9999409 1.5457849e-07
Epoch: [161][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0594 (0.0769)	
0.99988866 7.526444e-07
Epoch: [161][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1315 (0.0732)	
0.9999491 1.7555472e-07
loss:  0.056805561749065925 0.05337699209870872
===========>   training    <===========
Epoch: [162][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1832 (0.1832)	
0.99990976 5.7340472e-08
===========>   testing    <===========
Epoch: [162][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1111 (0.1111)	
0.9998996 1.08765775e-07
Epoch: [162][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1824 (0.0840)	
0.9998466 5.1716415e-07
Epoch: [162][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1995 (0.0771)	
0.9999317 3.7589422e-07
loss:  0.06029935857578117 0.05337699209870872
===========>   training    <===========
Epoch: [163][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1259 (0.1259)	
0.99996173 2.903087e-06
===========>   testing    <===========
Epoch: [163][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0734 (0.0734)	
0.99992573 1.2583044e-07
Epoch: [163][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1038 (0.0745)	
0.99961424 1.9493272e-07
Epoch: [163][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0678 (0.0716)	
0.99992585 2.2404419e-07
loss:  0.055622874445912296 0.05337699209870872
===========>   training    <===========
Epoch: [164][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1948 (0.1948)	
0.9999368 1.9937884e-08
===========>   testing    <===========
Epoch: [164][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0977 (0.0977)	
0.9999572 7.7664316e-07
Epoch: [164][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1001 (0.0800)	
0.9998989 4.534348e-07
Epoch: [164][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1556 (0.0802)	
0.99995756 5.562241e-07
loss:  0.06189442687680302 0.05337699209870872
===========>   training    <===========
Epoch: [165][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1463 (0.1463)	
0.9999616 3.3298664e-07
===========>   testing    <===========
Epoch: [165][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0757 (0.0757)	
0.99988854 5.045341e-07
Epoch: [165][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.2103 (0.0778)	
0.99974483 7.447077e-07
Epoch: [165][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0960 (0.0717)	
0.9999311 3.3572636e-07
loss:  0.05763339314467131 0.05337699209870872
===========>   training    <===========
Epoch: [166][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1897 (0.1897)	
0.9999645 1.946971e-06
===========>   testing    <===========
Epoch: [166][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0691 (0.0691)	
0.999961 7.18144e-07
Epoch: [166][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0984 (0.0795)	
0.9999168 2.8020833e-07
Epoch: [166][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0842 (0.0772)	
0.99997497 2.098363e-07
loss:  0.056856710692152035 0.05337699209870872
===========>   training    <===========
Epoch: [167][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1408 (0.1408)	
0.99997747 2.374548e-07
===========>   testing    <===========
Epoch: [167][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0726 (0.0726)	
0.9998555 5.388919e-07
Epoch: [167][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0386 (0.0799)	
0.9997737 1.2303474e-07
Epoch: [167][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0857 (0.0755)	
0.9999249 4.6863798e-07
loss:  0.0571890875615525 0.05337699209870872
===========>   training    <===========
Epoch: [168][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1750 (0.1750)	
0.9999399 1.7455335e-05
===========>   testing    <===========
Epoch: [168][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1060 (0.1060)	
0.99990666 2.2817564e-07
Epoch: [168][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0851 (0.0789)	
0.9998934 2.4204306e-07
Epoch: [168][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1175 (0.0727)	
0.9999443 5.340888e-07
loss:  0.0580328443017587 0.05337699209870872
===========>   training    <===========
Epoch: [169][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1615 (0.1615)	
0.9999379 0.00030671086
===========>   testing    <===========
Epoch: [169][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1350 (0.1350)	
0.9999162 9.013883e-09
Epoch: [169][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.2010 (0.0813)	
0.99985766 2.3619393e-09
Epoch: [169][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0587 (0.0786)	
0.9999461 9.7370405e-09
loss:  0.060172240582189285 0.05337699209870872
===========>   training    <===========
Epoch: [170][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1635 (0.1635)	
0.99990463 1.665936e-08
===========>   testing    <===========
Epoch: [170][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0792 (0.0792)	
0.9999031 1.6969582e-07
Epoch: [170][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0733 (0.0709)	
0.9998983 2.610777e-08
Epoch: [170][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0924 (0.0654)	
0.99993885 1.268009e-07
loss:  0.05023974576168644 0.05337699209870872
===========>   training    <===========
Epoch: [171][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1207 (0.1207)	
0.9999684 1.785379e-06
===========>   testing    <===========
Epoch: [171][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0647 (0.0647)	
0.99988055 4.977977e-09
Epoch: [171][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1798 (0.0738)	
0.99982625 3.5092064e-08
Epoch: [171][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1587 (0.0715)	
0.9999161 1.24407356e-08
loss:  0.05528309382359142 0.05023974576168644
===========>   training    <===========
Epoch: [172][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1484 (0.1484)	
0.9999347 1.7587443e-07
===========>   testing    <===========
Epoch: [172][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0710 (0.0710)	
0.99994075 7.199736e-08
Epoch: [172][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.3143 (0.0796)	
0.9998265 6.25728e-08
Epoch: [172][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1103 (0.0708)	
0.9999474 4.3572438e-08
loss:  0.05403667652652833 0.05023974576168644
===========>   training    <===========
Epoch: [173][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1535 (0.1535)	
0.99992573 2.9744324e-08
===========>   testing    <===========
Epoch: [173][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0531 (0.0531)	
0.9999329 7.4284365e-08
Epoch: [173][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0542 (0.0699)	
0.9997917 2.2281938e-07
Epoch: [173][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1209 (0.0660)	
0.9999404 3.8383366e-08
loss:  0.05173616460875674 0.05023974576168644
===========>   training    <===========
Epoch: [174][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1373 (0.1373)	
0.9999505 2.7623057e-07
===========>   testing    <===========
Epoch: [174][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0522 (0.0522)	
0.99991643 2.1133246e-07
Epoch: [174][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0623 (0.0795)	
0.9998797 4.0187814e-07
Epoch: [174][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1685 (0.0779)	
0.99992085 3.3265783e-07
loss:  0.057384233803140505 0.05023974576168644
===========>   training    <===========
Epoch: [175][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1802 (0.1802)	
0.99992394 3.0962616e-07
===========>   testing    <===========
Epoch: [175][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0627 (0.0627)	
0.99991024 6.5202073e-07
Epoch: [175][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1154 (0.0842)	
0.9996718 2.5360552e-07
Epoch: [175][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.2135 (0.0758)	
0.9999198 1.0304667e-06
loss:  0.055706811357143504 0.05023974576168644
===========>   training    <===========
Epoch: [176][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1456 (0.1456)	
0.9999629 4.4098252e-07
===========>   testing    <===========
Epoch: [176][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0788 (0.0788)	
0.9999298 2.268431e-06
Epoch: [176][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1008 (0.0810)	
0.99990094 5.6968514e-07
Epoch: [176][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1023 (0.0745)	
0.9999486 2.1762717e-06
loss:  0.05528233836808605 0.05023974576168644
===========>   training    <===========
Epoch: [177][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1752 (0.1752)	
0.99995005 1.1965562e-06
===========>   testing    <===========
Epoch: [177][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0714 (0.0714)	
0.9998642 7.09661e-07
Epoch: [177][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0801 (0.0772)	
0.9996111 2.6047346e-06
Epoch: [177][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1221 (0.0754)	
0.9999131 3.0683512e-07
loss:  0.05757805849456499 0.05023974576168644
===========>   training    <===========
Epoch: [178][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1602 (0.1602)	
0.9999665 3.1868144e-06
===========>   testing    <===========
Epoch: [178][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1005 (0.1005)	
0.999879 9.4030383e-07
Epoch: [178][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0346 (0.0761)	
0.9996251 3.5478345e-06
Epoch: [178][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1047 (0.0731)	
0.99991083 2.8412837e-06
loss:  0.0533547883809925 0.05023974576168644
===========>   training    <===========
Epoch: [179][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1471 (0.1471)	
0.9999386 8.4557047e-07
===========>   testing    <===========
Epoch: [179][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0573 (0.0573)	
0.9998522 8.971364e-07
Epoch: [179][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.1961 (0.0712)	
0.999537 6.75722e-07
Epoch: [179][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006634204312890623]	Loss 0.0594 (0.0662)	
0.9999219 2.488429e-07
loss:  0.05278686341467698 0.05023974576168644
===========>   training    <===========
Epoch: [180][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1091 (0.1091)	
0.9999645 3.6335973e-06
===========>   testing    <===========
Epoch: [180][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0645 (0.0645)	
0.9999337 1.5031852e-06
Epoch: [180][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0384 (0.0760)	
0.9998511 3.047061e-07
Epoch: [180][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0787 (0.0699)	
0.9999596 3.1476046e-07
loss:  0.05225837086801788 0.05023974576168644
===========>   training    <===========
Epoch: [181][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1390 (0.1390)	
0.99991846 1.6111386e-06
===========>   testing    <===========
Epoch: [181][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0600 (0.0600)	
0.99995077 5.4555215e-07
Epoch: [181][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.2002 (0.0792)	
0.99981374 5.5476736e-08
Epoch: [181][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0833 (0.0719)	
0.99995685 9.564914e-09
loss:  0.053293457597086924 0.05023974576168644
===========>   training    <===========
Epoch: [182][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1258 (0.1258)	
0.9998542 1.03384934e-07
===========>   testing    <===========
Epoch: [182][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0513 (0.0513)	
0.99993336 1.9135737e-07
Epoch: [182][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0857 (0.0729)	
0.9997136 9.915105e-08
Epoch: [182][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0939 (0.0669)	
0.9999311 2.4881151e-08
loss:  0.05072706213969269 0.05023974576168644
===========>   training    <===========
Epoch: [183][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1225 (0.1225)	
0.9999461 1.9336683e-06
===========>   testing    <===========
Epoch: [183][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0680 (0.0680)	
0.9999417 7.528454e-07
Epoch: [183][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1091 (0.0765)	
0.9998191 2.3637008e-07
Epoch: [183][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1406 (0.0721)	
0.9999523 1.5577987e-07
loss:  0.0548268188483676 0.05023974576168644
===========>   training    <===========
Epoch: [184][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0933 (0.0933)	
0.9999554 3.1632794e-07
===========>   testing    <===========
Epoch: [184][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0855 (0.0855)	
0.9998969 1.405176e-06
Epoch: [184][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1868 (0.0790)	
0.9997179 1.7372564e-06
Epoch: [184][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0716 (0.0724)	
0.9999504 1.3620578e-06
loss:  0.057235264789006535 0.05023974576168644
===========>   training    <===========
Epoch: [185][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.2061 (0.2061)	
0.99990976 2.565005e-07
===========>   testing    <===========
Epoch: [185][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0723 (0.0723)	
0.9998869 5.99117e-07
Epoch: [185][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0456 (0.0726)	
0.9997918 3.8999238e-07
Epoch: [185][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1524 (0.0706)	
0.99990094 2.8877187e-07
loss:  0.05373240121967238 0.05023974576168644
===========>   training    <===========
Epoch: [186][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1118 (0.1118)	
0.9998889 3.1252932e-07
===========>   testing    <===========
Epoch: [186][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0844 (0.0844)	
0.99985516 4.3202607e-07
Epoch: [186][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0304 (0.0750)	
0.99969554 6.693494e-07
Epoch: [186][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1216 (0.0712)	
0.9999341 5.9889425e-07
loss:  0.05405595682238362 0.05023974576168644
===========>   training    <===========
Epoch: [187][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1660 (0.1660)	
0.999948 2.6025743e-06
===========>   testing    <===========
Epoch: [187][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0874 (0.0874)	
0.9999168 5.9556914e-08
Epoch: [187][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0799 (0.0745)	
0.9998037 2.8151588e-08
Epoch: [187][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0747 (0.0695)	
0.9999423 1.1204508e-08
loss:  0.05337136577641044 0.05023974576168644
===========>   training    <===========
Epoch: [188][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1524 (0.1524)	
0.99986625 4.7201993e-07
===========>   testing    <===========
Epoch: [188][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0597 (0.0597)	
0.9999485 8.291287e-07
Epoch: [188][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0346 (0.0700)	
0.99982566 4.1595598e-07
Epoch: [188][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1090 (0.0664)	
0.99995863 5.379358e-07
loss:  0.049546117731401274 0.05023974576168644
===========>   training    <===========
Epoch: [189][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1751 (0.1751)	
0.9999002 6.17109e-07
===========>   testing    <===========
Epoch: [189][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1373 (0.1373)	
0.9999479 2.821535e-07
Epoch: [189][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1449 (0.0817)	
0.9998317 3.898499e-08
Epoch: [189][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1340 (0.0782)	
0.99995375 8.743827e-08
loss:  0.05567887018849427 0.049546117731401274
===========>   training    <===========
Epoch: [190][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1452 (0.1452)	
0.9999697 8.701602e-06
===========>   testing    <===========
Epoch: [190][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0861 (0.0861)	
0.9999708 3.455197e-07
Epoch: [190][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0873 (0.0764)	
0.9999511 1.262213e-07
Epoch: [190][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0944 (0.0728)	
0.99998283 2.4834222e-07
loss:  0.05495749080326906 0.049546117731401274
===========>   training    <===========
Epoch: [191][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1446 (0.1446)	
0.9999734 4.5669532e-07
===========>   testing    <===========
Epoch: [191][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0416 (0.0416)	
0.9999716 4.0359228e-07
Epoch: [191][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.2814 (0.0777)	
0.9999516 5.2491674e-07
Epoch: [191][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1082 (0.0754)	
0.9999777 3.0569458e-07
loss:  0.054923786766525096 0.049546117731401274
===========>   training    <===========
Epoch: [192][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1281 (0.1281)	
0.9999784 3.8312498e-07
===========>   testing    <===========
Epoch: [192][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0513 (0.0513)	
0.99994385 2.2603758e-06
Epoch: [192][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.2604 (0.0795)	
0.9998709 2.294829e-06
Epoch: [192][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1268 (0.0772)	
0.99995816 2.0413363e-06
loss:  0.05987695451051522 0.049546117731401274
===========>   training    <===========
Epoch: [193][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1705 (0.1705)	
0.9999434 1.0590526e-06
===========>   testing    <===========
Epoch: [193][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0692 (0.0692)	
0.9999249 5.627236e-09
Epoch: [193][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1467 (0.0707)	
0.9998072 1.0731338e-08
Epoch: [193][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0912 (0.0689)	
0.9999604 6.852991e-10
loss:  0.05406034683706129 0.049546117731401274
===========>   training    <===========
Epoch: [194][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1487 (0.1487)	
0.9999598 6.6668896e-08
===========>   testing    <===========
Epoch: [194][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0891 (0.0891)	
0.99994576 1.8338886e-07
Epoch: [194][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0470 (0.0761)	
0.9998419 1.0003606e-07
Epoch: [194][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0891 (0.0720)	
0.9999678 1.7222601e-07
loss:  0.055293842986212094 0.049546117731401274
===========>   training    <===========
Epoch: [195][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1183 (0.1183)	
0.9999627 3.135354e-07
===========>   testing    <===========
Epoch: [195][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0559 (0.0559)	
0.9999361 8.401528e-07
Epoch: [195][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.3208 (0.0725)	
0.99988925 2.5201737e-07
Epoch: [195][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0480 (0.0693)	
0.99994683 3.491368e-07
loss:  0.05312797537249969 0.049546117731401274
===========>   training    <===========
Epoch: [196][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1778 (0.1778)	
0.9999455 9.31804e-08
===========>   testing    <===========
Epoch: [196][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0536 (0.0536)	
0.9999229 9.4055946e-07
Epoch: [196][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.3591 (0.0774)	
0.9998259 6.625425e-08
Epoch: [196][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1439 (0.0730)	
0.9999665 7.238718e-08
loss:  0.05590147376263499 0.049546117731401274
===========>   training    <===========
Epoch: [197][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1102 (0.1102)	
0.99998486 1.7722286e-06
===========>   testing    <===========
Epoch: [197][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0804 (0.0804)	
0.9998932 4.7618296e-09
Epoch: [197][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.3260 (0.0775)	
0.9998696 1.232627e-09
Epoch: [197][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1784 (0.0749)	
0.99994504 1.1038707e-09
loss:  0.05598257780747351 0.049546117731401274
===========>   training    <===========
Epoch: [198][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1362 (0.1362)	
0.9999602 1.1265689e-08
===========>   testing    <===========
Epoch: [198][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1557 (0.1557)	
0.999897 8.5489866e-08
Epoch: [198][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.2867 (0.0781)	
0.99984694 6.7963995e-08
Epoch: [198][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0316 (0.0730)	
0.9999554 1.2552121e-07
loss:  0.057427786028037664 0.049546117731401274
===========>   training    <===========
Epoch: [199][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.1666 (0.1666)	
0.9999367 1.3106926e-07
===========>   testing    <===========
Epoch: [199][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0894 (0.0894)	
0.99993694 1.9335637e-07
Epoch: [199][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0816 (0.0752)	
0.99989057 2.2631274e-07
Epoch: [199][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0006302494097246091]	Loss 0.0789 (0.0735)	
0.9999713 5.661527e-07
loss:  0.0553066126118501 0.049546117731401274
===========>   training    <===========
Epoch: [200][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1004 (0.1004)	
0.99997556 4.013339e-07
===========>   testing    <===========
Epoch: [200][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0625 (0.0625)	
0.99996483 6.574831e-08
Epoch: [200][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1303 (0.0703)	
0.9999088 2.591832e-07
Epoch: [200][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0960 (0.0686)	
0.99998236 2.6343275e-07
loss:  0.05270539708445976 0.049546117731401274
===========>   training    <===========
Epoch: [201][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1159 (0.1159)	
0.99997675 1.4996813e-07
===========>   testing    <===========
Epoch: [201][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0740 (0.0740)	
0.99996674 1.3859444e-07
Epoch: [201][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0753 (0.0754)	
0.9998927 2.2711293e-07
Epoch: [201][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0973 (0.0691)	
0.99997437 3.5580703e-07
loss:  0.05170019942597015 0.049546117731401274
===========>   training    <===========
Epoch: [202][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1718 (0.1718)	
0.99994576 2.1088029e-07
===========>   testing    <===========
Epoch: [202][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0874 (0.0874)	
0.99994206 5.8977346e-08
Epoch: [202][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0281 (0.0727)	
0.9998448 4.5595637e-08
Epoch: [202][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0863 (0.0689)	
0.9999702 1.6978543e-08
loss:  0.05319726385499968 0.049546117731401274
===========>   training    <===========
Epoch: [203][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1623 (0.1623)	
0.9999771 1.9098528e-07
===========>   testing    <===========
Epoch: [203][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0894 (0.0894)	
0.999959 4.79988e-07
Epoch: [203][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0971 (0.0736)	
0.9999136 3.0501428e-07
Epoch: [203][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0460 (0.0669)	
0.999977 8.996365e-07
loss:  0.05323547915809068 0.049546117731401274
===========>   training    <===========
Epoch: [204][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1206 (0.1206)	
0.99997103 2.0917496e-07
===========>   testing    <===========
Epoch: [204][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0495 (0.0495)	
0.9999373 2.7547551e-07
Epoch: [204][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.3423 (0.0741)	
0.9999263 1.791651e-07
Epoch: [204][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.2049 (0.0714)	
0.99996746 1.5970512e-07
loss:  0.052328491426402834 0.049546117731401274
===========>   training    <===========
Epoch: [205][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1141 (0.1141)	
0.9999796 2.4600579e-06
===========>   testing    <===========
Epoch: [205][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0953 (0.0953)	
0.99988544 3.8992545e-07
Epoch: [205][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.2732 (0.0785)	
0.9998871 4.5442746e-08
Epoch: [205][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0717 (0.0727)	
0.9999471 2.1731715e-08
loss:  0.05407652855178602 0.049546117731401274
===========>   training    <===========
Epoch: [206][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1404 (0.1404)	
0.9999598 2.4458575e-07
===========>   testing    <===========
Epoch: [206][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1371 (0.1371)	
0.9999746 1.7616686e-07
Epoch: [206][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.2901 (0.0765)	
0.99993825 2.5598014e-08
Epoch: [206][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1424 (0.0730)	
0.999982 3.1157164e-08
loss:  0.05122410519811971 0.049546117731401274
===========>   training    <===========
Epoch: [207][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1081 (0.1081)	
0.9999778 1.05592456e-07
===========>   testing    <===========
Epoch: [207][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0725 (0.0725)	
0.99995637 2.54955e-07
Epoch: [207][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.4025 (0.0759)	
0.999897 1.6576894e-07
Epoch: [207][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0659 (0.0693)	
0.99997723 1.941609e-07
loss:  0.05415238968478475 0.049546117731401274
===========>   training    <===========
Epoch: [208][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1432 (0.1432)	
0.99996805 1.9923597e-07
===========>   testing    <===========
Epoch: [208][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1064 (0.1064)	
0.9999682 7.53655e-07
Epoch: [208][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.2526 (0.0774)	
0.9999274 5.195172e-07
Epoch: [208][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0806 (0.0701)	
0.9999733 2.6035102e-06
loss:  0.05412027974745326 0.049546117731401274
===========>   training    <===========
Epoch: [209][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1337 (0.1337)	
0.99996126 6.8615503e-07
===========>   testing    <===========
Epoch: [209][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0997 (0.0997)	
0.9999548 1.2283613e-07
Epoch: [209][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1224 (0.0771)	
0.99984753 4.063257e-08
Epoch: [209][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0837 (0.0711)	
0.99996114 1.2301702e-07
loss:  0.05219334642323692 0.049546117731401274
===========>   training    <===========
Epoch: [210][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1293 (0.1293)	
0.9999709 1.0668174e-06
===========>   testing    <===========
Epoch: [210][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0653 (0.0653)	
0.9999516 3.0828926e-06
Epoch: [210][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1081 (0.0715)	
0.99991405 4.5124898e-07
Epoch: [210][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1482 (0.0681)	
0.9999658 3.2748314e-06
loss:  0.0512023137940506 0.049546117731401274
===========>   training    <===========
Epoch: [211][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1579 (0.1579)	
0.99996305 2.2914305e-06
===========>   testing    <===========
Epoch: [211][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0942 (0.0942)	
0.9999478 2.2735065e-07
Epoch: [211][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1616 (0.0736)	
0.9998945 1.6293829e-07
Epoch: [211][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1227 (0.0698)	
0.9999471 1.3922742e-07
loss:  0.052488179736660645 0.049546117731401274
===========>   training    <===========
Epoch: [212][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1819 (0.1819)	
0.99997985 2.1811222e-07
===========>   testing    <===========
Epoch: [212][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1096 (0.1096)	
0.99997413 9.2113055e-07
Epoch: [212][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.2391 (0.0747)	
0.99995875 4.0319756e-07
Epoch: [212][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1084 (0.0709)	
0.9999819 7.875542e-07
loss:  0.052994901854960474 0.049546117731401274
===========>   training    <===========
Epoch: [213][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1072 (0.1072)	
0.999936 8.383522e-08
===========>   testing    <===========
Epoch: [213][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1705 (0.1705)	
0.9999664 3.087533e-06
Epoch: [213][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0945 (0.0784)	
0.9999107 3.8238542e-07
Epoch: [213][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1223 (0.0734)	
0.9999684 1.0524918e-06
loss:  0.05551408378144851 0.049546117731401274
===========>   training    <===========
Epoch: [214][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1915 (0.1915)	
0.99990785 9.63411e-08
===========>   testing    <===========
Epoch: [214][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.2019 (0.2019)	
0.99995184 1.0654895e-06
Epoch: [214][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1287 (0.0688)	
0.99989676 1.7203772e-07
Epoch: [214][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1277 (0.0670)	
0.99996305 9.449535e-08
loss:  0.05125119209053064 0.049546117731401274
===========>   training    <===========
Epoch: [215][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1698 (0.1698)	
0.99990237 6.2141e-07
===========>   testing    <===========
Epoch: [215][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0953 (0.0953)	
0.9999362 3.3608208e-06
Epoch: [215][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1038 (0.0689)	
0.9998578 1.7870758e-06
Epoch: [215][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0809 (0.0643)	
0.99994946 2.505441e-06
loss:  0.04804997635613739 0.049546117731401274
===========>   training    <===========
Epoch: [216][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1328 (0.1328)	
0.9999659 8.08687e-07
===========>   testing    <===========
Epoch: [216][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0741 (0.0741)	
0.9999281 3.6182365e-07
Epoch: [216][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0526 (0.0710)	
0.999876 4.4716128e-08
Epoch: [216][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1204 (0.0669)	
0.9999751 7.725781e-08
loss:  0.049640331368262736 0.04804997635613739
===========>   training    <===========
Epoch: [217][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1814 (0.1814)	
0.9999578 2.385336e-07
===========>   testing    <===========
Epoch: [217][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0557 (0.0557)	
0.99994504 3.6935816e-07
Epoch: [217][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0771 (0.0687)	
0.9998944 1.5282816e-07
Epoch: [217][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1059 (0.0659)	
0.9999727 6.923793e-07
loss:  0.04925127693384701 0.04804997635613739
===========>   training    <===========
Epoch: [218][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1252 (0.1252)	
0.9999659 2.9208778e-07
===========>   testing    <===========
Epoch: [218][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1191 (0.1191)	
0.9997842 1.00468796e-07
Epoch: [218][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0440 (0.0711)	
0.99986446 5.2118352e-08
Epoch: [218][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1009 (0.0685)	
0.9999621 5.0192718e-08
loss:  0.050121868248121704 0.04804997635613739
===========>   training    <===========
Epoch: [219][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1626 (0.1626)	
0.9999597 6.036127e-07
===========>   testing    <===========
Epoch: [219][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.1223 (0.1223)	
0.99996006 3.8157015e-07
Epoch: [219][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0586 (0.0770)	
0.9999378 2.3368158e-07
Epoch: [219][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005987369392383787]	Loss 0.0632 (0.0687)	
0.9999722 9.792248e-08
loss:  0.05465670070294415 0.04804997635613739
===========>   training    <===========
Epoch: [220][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1270 (0.1270)	
0.9999418 3.9851912e-08
===========>   testing    <===========
Epoch: [220][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0562 (0.0562)	
0.9999566 8.817422e-07
Epoch: [220][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0282 (0.0684)	
0.9999095 6.136757e-07
Epoch: [220][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1584 (0.0639)	
0.9999703 1.3883497e-06
loss:  0.04954302889943629 0.04804997635613739
===========>   training    <===========
Epoch: [221][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1393 (0.1393)	
0.99994504 6.481942e-08
===========>   testing    <===========
Epoch: [221][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0861 (0.0861)	
0.9999044 3.6497076e-07
Epoch: [221][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0358 (0.0689)	
0.9998388 3.1922514e-07
Epoch: [221][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1344 (0.0643)	
0.99996376 2.5724816e-07
loss:  0.04827279888996061 0.04804997635613739
===========>   training    <===========
Epoch: [222][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1550 (0.1550)	
0.9999695 2.670143e-07
===========>   testing    <===========
Epoch: [222][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1477 (0.1477)	
0.99997795 9.788482e-07
Epoch: [222][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0781 (0.0724)	
0.9999491 9.5075654e-07
Epoch: [222][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0703 (0.0687)	
0.9999864 4.0265996e-07
loss:  0.05163041233508192 0.04804997635613739
===========>   training    <===========
Epoch: [223][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1350 (0.1350)	
0.99996126 2.450845e-07
===========>   testing    <===========
Epoch: [223][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1052 (0.1052)	
0.99996245 9.040729e-08
Epoch: [223][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1033 (0.0728)	
0.9999404 8.7622254e-08
Epoch: [223][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1272 (0.0697)	
0.9999833 2.8851193e-08
loss:  0.05199156569177765 0.04804997635613739
===========>   training    <===========
Epoch: [224][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1305 (0.1305)	
0.99996924 1.4169769e-08
===========>   testing    <===========
Epoch: [224][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0574 (0.0574)	
0.99994445 8.4468574e-08
Epoch: [224][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0317 (0.0687)	
0.999806 5.542766e-08
Epoch: [224][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1677 (0.0649)	
0.99995863 2.24142e-08
loss:  0.05089604332005515 0.04804997635613739
===========>   training    <===========
Epoch: [225][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1212 (0.1212)	
0.9999448 2.7842304e-07
===========>   testing    <===========
Epoch: [225][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0586 (0.0586)	
0.9999598 6.0559097e-07
Epoch: [225][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0604 (0.0693)	
0.99993074 5.081766e-07
Epoch: [225][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1208 (0.0672)	
0.99996567 5.324817e-07
loss:  0.04887300614166379 0.04804997635613739
===========>   training    <===========
Epoch: [226][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0966 (0.0966)	
0.9999871 1.2708869e-06
===========>   testing    <===========
Epoch: [226][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0772 (0.0772)	
0.99995744 1.311414e-07
Epoch: [226][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0606 (0.0699)	
0.9998286 1.9485651e-07
Epoch: [226][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0667 (0.0636)	
0.99997973 2.489525e-08
loss:  0.04916373798722884 0.04804997635613739
===========>   training    <===========
Epoch: [227][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1266 (0.1266)	
0.9999716 9.632821e-07
===========>   testing    <===========
Epoch: [227][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0833 (0.0833)	
0.9999707 1.1476807e-07
Epoch: [227][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0274 (0.0697)	
0.999902 3.9759968e-08
Epoch: [227][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1102 (0.0652)	
0.9999732 2.8389382e-08
loss:  0.05008044912636911 0.04804997635613739
===========>   training    <===========
Epoch: [228][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1376 (0.1376)	
0.99996996 2.5969504e-08
===========>   testing    <===========
Epoch: [228][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0551 (0.0551)	
0.99996984 4.7889205e-07
Epoch: [228][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0688 (0.0658)	
0.999948 2.6203512e-07
Epoch: [228][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1657 (0.0620)	
0.9999821 2.1676496e-07
loss:  0.0470722320354926 0.04804997635613739
===========>   training    <===========
Epoch: [229][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1236 (0.1236)	
0.9999604 2.7885994e-07
===========>   testing    <===========
Epoch: [229][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0656 (0.0656)	
0.9999722 4.6629387e-07
Epoch: [229][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0327 (0.0681)	
0.99989927 3.0672777e-07
Epoch: [229][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1618 (0.0654)	
0.99996376 3.4729442e-07
loss:  0.04969738209700958 0.0470722320354926
===========>   training    <===========
Epoch: [230][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1238 (0.1238)	
0.9999677 1.3925477e-07
===========>   testing    <===========
Epoch: [230][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0695 (0.0695)	
0.9999801 2.9489232e-07
Epoch: [230][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0302 (0.0736)	
0.9999014 4.2963356e-07
Epoch: [230][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1500 (0.0698)	
0.9999865 2.4212918e-07
loss:  0.053133371128155416 0.0470722320354926
===========>   training    <===========
Epoch: [231][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1394 (0.1394)	
0.99997973 1.4941594e-07
===========>   testing    <===========
Epoch: [231][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0661 (0.0661)	
0.99994266 5.037062e-08
Epoch: [231][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0431 (0.0695)	
0.99984646 7.368155e-08
Epoch: [231][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.2213 (0.0655)	
0.99997187 4.8170072e-08
loss:  0.04948880117536736 0.0470722320354926
===========>   training    <===========
Epoch: [232][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1060 (0.1060)	
0.99996436 3.6645096e-07
===========>   testing    <===========
Epoch: [232][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0819 (0.0819)	
0.9999372 3.0412374e-07
Epoch: [232][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0705 (0.0735)	
0.99984837 3.6260263e-07
Epoch: [232][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1020 (0.0664)	
0.9999584 3.1403872e-07
loss:  0.04977523490548863 0.0470722320354926
===========>   training    <===========
Epoch: [233][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1157 (0.1157)	
0.99996066 2.0193468e-07
===========>   testing    <===========
Epoch: [233][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0436 (0.0436)	
0.9999405 5.748184e-08
Epoch: [233][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0854 (0.0662)	
0.9997662 1.1864994e-07
Epoch: [233][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0879 (0.0622)	
0.9999653 1.0321845e-07
loss:  0.047888343270883094 0.0470722320354926
===========>   training    <===========
Epoch: [234][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1622 (0.1622)	
0.9999118 1.8798595e-07
===========>   testing    <===========
Epoch: [234][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0909 (0.0909)	
0.9999665 7.544669e-08
Epoch: [234][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0933 (0.0703)	
0.9999466 2.6615422e-07
Epoch: [234][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1514 (0.0666)	
0.999982 1.14160045e-07
loss:  0.04837336502408973 0.0470722320354926
===========>   training    <===========
Epoch: [235][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1145 (0.1145)	
0.9999628 8.8006794e-07
===========>   testing    <===========
Epoch: [235][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0679 (0.0679)	
0.9999654 1.10500494e-07
Epoch: [235][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.2556 (0.0706)	
0.9998944 1.9740554e-07
Epoch: [235][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.2365 (0.0699)	
0.9999745 1.99851e-07
loss:  0.05097298916900206 0.0470722320354926
===========>   training    <===========
Epoch: [236][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1281 (0.1281)	
0.9999523 7.715399e-08
===========>   testing    <===========
Epoch: [236][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0721 (0.0721)	
0.99996305 7.052876e-08
Epoch: [236][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0603 (0.0733)	
0.99984765 1.3095219e-07
Epoch: [236][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.2477 (0.0708)	
0.99997294 7.83811e-08
loss:  0.053437144513141965 0.0470722320354926
===========>   training    <===========
Epoch: [237][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1301 (0.1301)	
0.9998609 1.7682107e-06
===========>   testing    <===========
Epoch: [237][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0834 (0.0834)	
0.9999771 1.1839963e-06
Epoch: [237][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0685 (0.0708)	
0.99991214 6.179417e-07
Epoch: [237][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1247 (0.0675)	
0.9999821 4.670576e-07
loss:  0.052579718628501304 0.0470722320354926
===========>   training    <===========
Epoch: [238][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1386 (0.1386)	
0.99994016 2.9540345e-07
===========>   testing    <===========
Epoch: [238][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0965 (0.0965)	
0.9999442 1.37991405e-08
Epoch: [238][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0620 (0.0698)	
0.9998443 2.8390572e-08
Epoch: [238][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0827 (0.0677)	
0.9999515 8.289723e-08
loss:  0.05103984882997581 0.0470722320354926
===========>   training    <===========
Epoch: [239][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1899 (0.1899)	
0.9999757 3.5867015e-07
===========>   testing    <===========
Epoch: [239][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0810 (0.0810)	
0.9999654 3.965504e-07
Epoch: [239][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.0966 (0.0677)	
0.99986684 1.7487196e-07
Epoch: [239][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005688000922764596]	Loss 0.1309 (0.0648)	
0.99997187 6.5452025e-07
loss:  0.04897711746412681 0.0470722320354926
===========>   training    <===========
Epoch: [240][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1285 (0.1285)	
0.99994755 1.3063e-07
===========>   testing    <===========
Epoch: [240][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0480 (0.0480)	
0.99997294 3.760717e-07
Epoch: [240][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0940 (0.0707)	
0.99993527 1.5167542e-07
Epoch: [240][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0932 (0.0664)	
0.99998367 2.4078557e-07
loss:  0.05134628236364758 0.0470722320354926
===========>   training    <===========
Epoch: [241][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1680 (0.1680)	
0.9999112 2.2574152e-07
===========>   testing    <===========
Epoch: [241][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0712 (0.0712)	
0.99997115 1.5097494e-07
Epoch: [241][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0387 (0.0671)	
0.99990284 2.2895469e-07
Epoch: [241][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.2035 (0.0630)	
0.99998355 1.0122801e-07
loss:  0.04818351557888101 0.0470722320354926
===========>   training    <===========
Epoch: [242][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1350 (0.1350)	
0.9999815 6.441308e-08
===========>   testing    <===========
Epoch: [242][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0589 (0.0589)	
0.9999366 7.3403634e-09
Epoch: [242][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0286 (0.0685)	
0.99986327 9.409837e-09
Epoch: [242][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1543 (0.0655)	
0.99994504 1.9295143e-08
loss:  0.04879275934035443 0.0470722320354926
===========>   training    <===========
Epoch: [243][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1597 (0.1597)	
0.9999703 5.1392516e-07
===========>   testing    <===========
Epoch: [243][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1280 (0.1280)	
0.9999404 3.176784e-07
Epoch: [243][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0491 (0.0776)	
0.99990475 2.598814e-07
Epoch: [243][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1916 (0.0717)	
0.9999472 1.780233e-07
loss:  0.05414097416672248 0.0470722320354926
===========>   training    <===========
Epoch: [244][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1031 (0.1031)	
0.9999554 2.0127176e-07
===========>   testing    <===========
Epoch: [244][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0873 (0.0873)	
0.9999535 1.12809914e-07
Epoch: [244][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1110 (0.0725)	
0.99986696 6.371081e-07
Epoch: [244][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0989 (0.0660)	
0.9999782 4.8030163e-08
loss:  0.052559115974615156 0.0470722320354926
===========>   training    <===========
Epoch: [245][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1294 (0.1294)	
0.9999664 7.449294e-08
===========>   testing    <===========
Epoch: [245][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0835 (0.0835)	
0.99993527 2.879215e-08
Epoch: [245][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0405 (0.0663)	
0.9998223 8.396339e-08
Epoch: [245][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.2165 (0.0626)	
0.9999689 2.1374282e-08
loss:  0.04742005326423526 0.0470722320354926
===========>   training    <===========
Epoch: [246][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1391 (0.1391)	
0.9999442 1.3365079e-07
===========>   testing    <===========
Epoch: [246][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1056 (0.1056)	
0.999907 4.8605227e-08
Epoch: [246][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0231 (0.0653)	
0.99984014 4.6016922e-08
Epoch: [246][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1669 (0.0621)	
0.99996567 4.364064e-08
loss:  0.04689977706244797 0.0470722320354926
===========>   training    <===========
Epoch: [247][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1098 (0.1098)	
0.9999764 2.3143491e-07
===========>   testing    <===========
Epoch: [247][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1134 (0.1134)	
0.99997497 1.8451841e-08
Epoch: [247][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0301 (0.0654)	
0.9998534 4.365146e-08
Epoch: [247][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1032 (0.0623)	
0.9999738 1.0361409e-08
loss:  0.04687357127391045 0.04689977706244797
===========>   training    <===========
Epoch: [248][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1228 (0.1228)	
0.9999608 7.336395e-11
===========>   testing    <===========
Epoch: [248][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0731 (0.0731)	
0.9999732 6.8462946e-08
Epoch: [248][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0513 (0.0714)	
0.9999399 9.623458e-08
Epoch: [248][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1522 (0.0670)	
0.9999759 5.3006794e-08
loss:  0.04868081493390697 0.04687357127391045
===========>   training    <===========
Epoch: [249][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1253 (0.1253)	
0.9999589 5.0954554e-08
===========>   testing    <===========
Epoch: [249][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0508 (0.0508)	
0.9999815 9.718286e-08
Epoch: [249][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0197 (0.0686)	
0.99996173 2.3025645e-07
Epoch: [249][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1937 (0.0649)	
0.99998975 7.381391e-08
loss:  0.04692443527771073 0.04687357127391045
===========>   training    <===========
Epoch: [250][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1233 (0.1233)	
0.9999503 3.159013e-10
===========>   testing    <===========
Epoch: [250][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0656 (0.0656)	
0.99996924 2.8019602e-07
Epoch: [250][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0318 (0.0637)	
0.9999187 1.3177881e-06
Epoch: [250][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1216 (0.0646)	
0.9999727 2.210877e-07
loss:  0.046895246345306396 0.04687357127391045
===========>   training    <===========
Epoch: [251][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1148 (0.1148)	
0.99997926 1.3128096e-06
===========>   testing    <===========
Epoch: [251][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0994 (0.0994)	
0.9999579 5.98345e-08
Epoch: [251][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0336 (0.0694)	
0.99984086 1.1738982e-07
Epoch: [251][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0573 (0.0662)	
0.9999715 7.0191916e-08
loss:  0.051308435070910985 0.04687357127391045
===========>   training    <===========
Epoch: [252][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1128 (0.1128)	
0.99998033 8.503627e-07
===========>   testing    <===========
Epoch: [252][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1415 (0.1415)	
0.9999753 6.5745176e-08
Epoch: [252][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0308 (0.0697)	
0.9999387 4.1730445e-08
Epoch: [252][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0781 (0.0686)	
0.99998105 8.0539316e-08
loss:  0.05223797479855308 0.04687357127391045
===========>   training    <===========
Epoch: [253][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1255 (0.1255)	
0.99993396 2.611286e-06
===========>   testing    <===========
Epoch: [253][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1464 (0.1464)	
0.99995613 3.562728e-08
Epoch: [253][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0460 (0.0649)	
0.999944 1.0407166e-07
Epoch: [253][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1434 (0.0632)	
0.9999683 5.2828145e-08
loss:  0.04802726389755185 0.04687357127391045
===========>   training    <===========
Epoch: [254][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1024 (0.1024)	
0.99995947 6.32895e-07
===========>   testing    <===========
Epoch: [254][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0997 (0.0997)	
0.999972 7.144171e-08
Epoch: [254][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0346 (0.0667)	
0.99993575 2.5547197e-07
Epoch: [254][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0668 (0.0658)	
0.99998546 9.823657e-08
loss:  0.048453991023514775 0.04687357127391045
===========>   training    <===========
Epoch: [255][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1226 (0.1226)	
0.9999645 1.5035005e-07
===========>   testing    <===========
Epoch: [255][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0921 (0.0921)	
0.999964 4.4055458e-08
Epoch: [255][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0286 (0.0661)	
0.9999237 1.3453008e-07
Epoch: [255][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1426 (0.0643)	
0.999984 6.756783e-08
loss:  0.05001902804239011 0.04687357127391045
===========>   training    <===========
Epoch: [256][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1474 (0.1474)	
0.99997675 4.141304e-08
===========>   testing    <===========
Epoch: [256][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0765 (0.0765)	
0.9999676 4.8713355e-07
Epoch: [256][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0274 (0.0728)	
0.99987817 2.870752e-07
Epoch: [256][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.2328 (0.0704)	
0.9999795 3.2458757e-07
loss:  0.052295090351371365 0.04687357127391045
===========>   training    <===========
Epoch: [257][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1143 (0.1143)	
0.9999783 7.61974e-07
===========>   testing    <===========
Epoch: [257][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0540 (0.0540)	
0.9999801 4.3905342e-07
Epoch: [257][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0318 (0.0646)	
0.9999466 7.7506644e-07
Epoch: [257][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1035 (0.0643)	
0.9999857 3.7059309e-07
loss:  0.048578010434061736 0.04687357127391045
===========>   training    <===========
Epoch: [258][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1488 (0.1488)	
0.99996877 2.5501288e-07
===========>   testing    <===========
Epoch: [258][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0448 (0.0448)	
0.9999765 9.0100876e-08
Epoch: [258][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0331 (0.0653)	
0.9998629 1.3086878e-07
Epoch: [258][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0715 (0.0625)	
0.99998784 8.575786e-08
loss:  0.04829676357472701 0.04687357127391045
===========>   training    <===========
Epoch: [259][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.1299 (0.1299)	
0.99998283 8.7342573e-07
===========>   testing    <===========
Epoch: [259][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0399 (0.0399)	
0.9998148 1.6119257e-07
Epoch: [259][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0466 (0.0661)	
0.9997218 3.5444356e-07
Epoch: [259][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005403600876626366]	Loss 0.0701 (0.0630)	
0.99997437 1.6499202e-07
loss:  0.04838632113538399 0.04687357127391045
===========>   training    <===========
Epoch: [260][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1387 (0.1387)	
0.999949 4.0042667e-07
===========>   testing    <===========
Epoch: [260][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0714 (0.0714)	
0.99993837 3.0539093e-07
Epoch: [260][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0330 (0.0687)	
0.9997756 3.849459e-07
Epoch: [260][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0465 (0.0663)	
0.9999653 2.4515697e-07
loss:  0.051456434963057074 0.04687357127391045
===========>   training    <===========
Epoch: [261][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1218 (0.1218)	
0.9999423 8.625393e-08
===========>   testing    <===========
Epoch: [261][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0592 (0.0592)	
0.99994135 8.7087415e-08
Epoch: [261][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0433 (0.0678)	
0.99987066 8.749784e-08
Epoch: [261][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0671 (0.0617)	
0.9999492 1.9664343e-07
loss:  0.04729649660454438 0.04687357127391045
===========>   training    <===========
Epoch: [262][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1225 (0.1225)	
0.999961 3.51336e-06
===========>   testing    <===========
Epoch: [262][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0709 (0.0709)	
0.9999684 1.9682447e-07
Epoch: [262][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0561 (0.0669)	
0.9999229 2.9426246e-07
Epoch: [262][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1620 (0.0641)	
0.99998295 2.368645e-07
loss:  0.05001278407616383 0.04687357127391045
===========>   training    <===========
Epoch: [263][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1366 (0.1366)	
0.9999387 3.5745013e-06
===========>   testing    <===========
Epoch: [263][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0558 (0.0558)	
0.99998116 1.5659396e-08
Epoch: [263][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0398 (0.0641)	
0.9999162 2.9558372e-08
Epoch: [263][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0796 (0.0620)	
0.9999838 2.8566836e-08
loss:  0.04667744253598316 0.04687357127391045
===========>   training    <===========
Epoch: [264][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1310 (0.1310)	
0.99996996 1.3536061e-08
===========>   testing    <===========
Epoch: [264][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0531 (0.0531)	
0.9999604 2.028666e-07
Epoch: [264][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0377 (0.0630)	
0.9997743 4.3396523e-07
Epoch: [264][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0467 (0.0628)	
0.9999733 2.0941448e-07
loss:  0.048311050155676605 0.04667744253598316
===========>   training    <===========
Epoch: [265][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1375 (0.1375)	
0.9999567 4.5003574e-07
===========>   testing    <===========
Epoch: [265][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0459 (0.0459)	
0.9999504 1.10581674e-07
Epoch: [265][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0313 (0.0605)	
0.9997749 2.454967e-07
Epoch: [265][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0513 (0.0596)	
0.99998116 1.2430245e-07
loss:  0.04684998984907962 0.04667744253598316
===========>   training    <===========
Epoch: [266][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1212 (0.1212)	
0.9999913 1.1225026e-07
===========>   testing    <===========
Epoch: [266][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0388 (0.0388)	
0.99997604 6.732534e-08
Epoch: [266][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0681 (0.0633)	
0.9998379 1.8639925e-07
Epoch: [266][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0598 (0.0613)	
0.99998724 6.728952e-08
loss:  0.04581874520481066 0.04667744253598316
===========>   training    <===========
Epoch: [267][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1875 (0.1875)	
0.9999676 1.2114492e-07
===========>   testing    <===========
Epoch: [267][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0421 (0.0421)	
0.9999765 7.184181e-08
Epoch: [267][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0836 (0.0629)	
0.9998647 2.6753327e-07
Epoch: [267][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0974 (0.0600)	
0.99998534 6.740088e-08
loss:  0.04728024843050549 0.04581874520481066
===========>   training    <===========
Epoch: [268][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1533 (0.1533)	
0.9999738 5.9053883e-07
===========>   testing    <===========
Epoch: [268][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0805 (0.0805)	
0.9999765 5.336072e-08
Epoch: [268][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0394 (0.0653)	
0.99993086 8.362839e-08
Epoch: [268][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1331 (0.0631)	
0.9999869 2.8490222e-08
loss:  0.04801590577681247 0.04581874520481066
===========>   training    <===========
Epoch: [269][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1613 (0.1613)	
0.9999918 3.5041472e-07
===========>   testing    <===========
Epoch: [269][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0654 (0.0654)	
0.99997973 8.485241e-08
Epoch: [269][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0610 (0.0630)	
0.99989486 1.1376748e-07
Epoch: [269][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0792 (0.0631)	
0.999979 1.04918314e-07
loss:  0.04807177766425197 0.04581874520481066
===========>   training    <===========
Epoch: [270][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1727 (0.1727)	
0.99994814 7.3771673e-07
===========>   testing    <===========
Epoch: [270][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0508 (0.0508)	
0.9999788 1.2805917e-07
Epoch: [270][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0480 (0.0651)	
0.9998691 1.710917e-07
Epoch: [270][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0790 (0.0618)	
0.99998474 1.3656707e-07
loss:  0.04633620928409854 0.04581874520481066
===========>   training    <===========
Epoch: [271][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1523 (0.1523)	
0.9999807 2.7056348e-07
===========>   testing    <===========
Epoch: [271][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0405 (0.0405)	
0.9999598 7.272915e-08
Epoch: [271][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0619 (0.0650)	
0.9998877 1.1414067e-07
Epoch: [271][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0496 (0.0628)	
0.9999862 6.673798e-08
loss:  0.051851465787708606 0.04581874520481066
===========>   training    <===========
Epoch: [272][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1200 (0.1200)	
0.99998236 2.8600212e-07
===========>   testing    <===========
Epoch: [272][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0602 (0.0602)	
0.99996364 2.619516e-08
Epoch: [272][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0427 (0.0669)	
0.9997893 1.8865292e-08
Epoch: [272][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0891 (0.0655)	
0.9999765 1.223992e-08
loss:  0.05024441085776199 0.04581874520481066
===========>   training    <===========
Epoch: [273][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1290 (0.1290)	
0.9999813 3.254809e-08
===========>   testing    <===========
Epoch: [273][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0401 (0.0401)	
0.99995697 1.565764e-07
Epoch: [273][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0226 (0.0609)	
0.9998543 3.4130937e-07
Epoch: [273][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1076 (0.0596)	
0.99996924 9.153792e-08
loss:  0.04715020892732835 0.04581874520481066
===========>   training    <===========
Epoch: [274][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0977 (0.0977)	
0.9999765 4.9302894e-08
===========>   testing    <===========
Epoch: [274][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0581 (0.0581)	
0.99997437 1.2934659e-07
Epoch: [274][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0382 (0.0643)	
0.99988794 4.0705183e-07
Epoch: [274][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0831 (0.0617)	
0.99997437 1.2206772e-07
loss:  0.04732133921455617 0.04581874520481066
===========>   training    <===========
Epoch: [275][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0950 (0.0950)	
0.9999716 8.880913e-08
===========>   testing    <===========
Epoch: [275][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0522 (0.0522)	
0.9999541 7.686126e-08
Epoch: [275][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0584 (0.0638)	
0.9999032 1.7580368e-07
Epoch: [275][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0730 (0.0622)	
0.9999684 6.4585926e-08
loss:  0.04930735451881285 0.04581874520481066
===========>   training    <===========
Epoch: [276][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1270 (0.1270)	
0.9999217 6.28774e-08
===========>   testing    <===========
Epoch: [276][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0503 (0.0503)	
0.9999751 5.1851234e-08
Epoch: [276][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0312 (0.0692)	
0.9999211 1.2454821e-07
Epoch: [276][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1367 (0.0650)	
0.9999881 5.2218653e-08
loss:  0.04970283380224261 0.04581874520481066
===========>   training    <===========
Epoch: [277][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1256 (0.1256)	
0.99998975 1.0628553e-07
===========>   testing    <===========
Epoch: [277][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0384 (0.0384)	
0.9999764 6.78319e-08
Epoch: [277][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0651 (0.0628)	
0.99990726 1.2005759e-07
Epoch: [277][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1094 (0.0600)	
0.9999856 6.776723e-08
loss:  0.044519936549281214 0.04581874520481066
===========>   training    <===========
Epoch: [278][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1196 (0.1196)	
0.99999356 2.44255e-07
===========>   testing    <===========
Epoch: [278][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1321 (0.1321)	
0.9999635 4.7544899e-07
Epoch: [278][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0427 (0.0656)	
0.9998869 7.3104724e-07
Epoch: [278][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.2053 (0.0638)	
0.9999784 3.2788827e-07
loss:  0.047347039235012045 0.044519936549281214
===========>   training    <===========
Epoch: [279][0/46]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.1262 (0.1262)	
0.999992 1.4176062e-07
===========>   testing    <===========
Epoch: [279][0/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0470 (0.0470)	
0.9999845 3.0559664e-07
Epoch: [279][100/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0258 (0.0636)	
0.99991524 4.726848e-07
Epoch: [279][200/289]	Lr-deconv: [0.0]	Lr-other: [0.0005133420832795048]	Loss 0.0799 (0.0599)	
0.9999857 3.1705824e-07
loss:  0.04406511487214693 0.044519936549281214
===========>   training    <===========
Epoch: [280][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1264 (0.1264)	
0.9999765 4.2772234e-07
===========>   testing    <===========
Epoch: [280][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0351 (0.0351)	
0.9999713 6.3767395e-08
Epoch: [280][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0350 (0.0630)	
0.9998814 8.051689e-08
Epoch: [280][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1060 (0.0607)	
0.9999794 4.359031e-08
loss:  0.045669318497958744 0.04406511487214693
===========>   training    <===========
Epoch: [281][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1030 (0.1030)	
0.9999548 6.435757e-07
===========>   testing    <===========
Epoch: [281][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0518 (0.0518)	
0.9999635 2.0989354e-07
Epoch: [281][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0244 (0.0637)	
0.9999187 2.1203884e-07
Epoch: [281][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1065 (0.0611)	
0.99998176 1.5640597e-07
loss:  0.04636581659088512 0.04406511487214693
===========>   training    <===========
Epoch: [282][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1199 (0.1199)	
0.9999763 6.211653e-07
===========>   testing    <===========
Epoch: [282][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0430 (0.0430)	
0.99996936 2.7561322e-07
Epoch: [282][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0223 (0.0613)	
0.9998845 1.6782776e-07
Epoch: [282][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1044 (0.0582)	
0.99997616 3.078167e-07
loss:  0.043041068380329284 0.04406511487214693
===========>   training    <===========
Epoch: [283][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1564 (0.1564)	
0.9999515 2.6831256e-07
===========>   testing    <===========
Epoch: [283][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0803 (0.0803)	
0.99991024 2.9516187e-07
Epoch: [283][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0310 (0.0665)	
0.9997869 2.5753312e-07
Epoch: [283][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1223 (0.0636)	
0.9999397 1.659119e-07
loss:  0.04653122750594252 0.043041068380329284
===========>   training    <===========
Epoch: [284][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1558 (0.1558)	
0.9999528 7.0701964e-07
===========>   testing    <===========
Epoch: [284][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0674 (0.0674)	
0.9998271 5.5013103e-08
Epoch: [284][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0315 (0.0651)	
0.99964905 8.360288e-08
Epoch: [284][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0992 (0.0629)	
0.9998098 3.3198653e-08
loss:  0.049071881182504384 0.043041068380329284
===========>   training    <===========
Epoch: [285][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1468 (0.1468)	
0.9997863 8.005815e-06
===========>   testing    <===========
Epoch: [285][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0451 (0.0451)	
0.9999032 7.139757e-08
Epoch: [285][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0280 (0.0659)	
0.99979275 8.4943245e-08
Epoch: [285][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0481 (0.0632)	
0.9999243 5.7051814e-08
loss:  0.04714752075631723 0.043041068380329284
===========>   training    <===========
Epoch: [286][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1625 (0.1625)	
0.99991906 6.203708e-07
===========>   testing    <===========
Epoch: [286][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0782 (0.0782)	
0.9999217 1.6971558e-07
Epoch: [286][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0298 (0.0657)	
0.99983513 2.6979845e-07
Epoch: [286][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0526 (0.0631)	
0.99993706 1.3739044e-07
loss:  0.04826715590662378 0.043041068380329284
===========>   training    <===========
Epoch: [287][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1079 (0.1079)	
0.999944 3.6564873e-07
===========>   testing    <===========
Epoch: [287][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0649 (0.0649)	
0.99995947 3.081168e-08
Epoch: [287][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0313 (0.0646)	
0.99985766 5.67593e-08
Epoch: [287][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1848 (0.0634)	
0.9999734 7.0573813e-09
loss:  0.04701101528963625 0.043041068380329284
===========>   training    <===========
Epoch: [288][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1150 (0.1150)	
0.999962 1.2132078e-07
===========>   testing    <===========
Epoch: [288][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0768 (0.0768)	
0.99993086 5.8841167e-08
Epoch: [288][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0236 (0.0697)	
0.99987745 2.5866885e-07
Epoch: [288][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1089 (0.0675)	
0.9999653 1.0677645e-07
loss:  0.04983654255260561 0.043041068380329284
===========>   training    <===========
Epoch: [289][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1264 (0.1264)	
0.99994993 7.625992e-08
===========>   testing    <===========
Epoch: [289][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0498 (0.0498)	
0.9999697 1.3739975e-07
Epoch: [289][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0377 (0.0692)	
0.9999244 6.016127e-07
Epoch: [289][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1373 (0.0658)	
0.99997294 1.103034e-07
loss:  0.05098300289937607 0.043041068380329284
===========>   training    <===========
Epoch: [290][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1677 (0.1677)	
0.99997866 9.234773e-07
===========>   testing    <===========
Epoch: [290][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0443 (0.0443)	
0.9999814 1.0645108e-07
Epoch: [290][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0365 (0.0698)	
0.9998264 2.2463574e-07
Epoch: [290][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1761 (0.0681)	
0.9999814 1.0628229e-07
loss:  0.05078151578869983 0.043041068380329284
===========>   training    <===========
Epoch: [291][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1069 (0.1069)	
0.99997747 4.5809905e-08
===========>   testing    <===========
Epoch: [291][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0428 (0.0428)	
0.99995995 6.548511e-08
Epoch: [291][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0622 (0.0635)	
0.999835 1.4290693e-07
Epoch: [291][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0595 (0.0623)	
0.9999646 6.667412e-08
loss:  0.04601657648913837 0.043041068380329284
===========>   training    <===========
Epoch: [292][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1407 (0.1407)	
0.9999713 3.0259096e-06
===========>   testing    <===========
Epoch: [292][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0756 (0.0756)	
0.9999367 3.3862158e-08
Epoch: [292][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0304 (0.0650)	
0.9997805 6.3003085e-08
Epoch: [292][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0877 (0.0630)	
0.999961 3.2466495e-08
loss:  0.046898904309837675 0.043041068380329284
===========>   training    <===========
Epoch: [293][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1237 (0.1237)	
0.99993277 3.4117852e-07
===========>   testing    <===========
Epoch: [293][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0700 (0.0700)	
0.99997675 2.0436569e-07
Epoch: [293][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1841 (0.0641)	
0.99990034 1.306623e-06
Epoch: [293][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1353 (0.0621)	
0.99997807 2.7201438e-07
loss:  0.04825484878721975 0.043041068380329284
===========>   training    <===========
Epoch: [294][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.2413 (0.2413)	
0.9999716 1.4330804e-07
===========>   testing    <===========
Epoch: [294][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0347 (0.0347)	
0.9999634 2.6386095e-07
Epoch: [294][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0295 (0.0668)	
0.9998957 3.1758478e-07
Epoch: [294][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0710 (0.0615)	
0.99997604 2.0836802e-07
loss:  0.045432429295244825 0.043041068380329284
===========>   training    <===========
Epoch: [295][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1195 (0.1195)	
0.9999727 2.258747e-06
===========>   testing    <===========
Epoch: [295][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0343 (0.0343)	
0.99996734 7.710383e-08
Epoch: [295][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0455 (0.0640)	
0.9998518 9.343704e-08
Epoch: [295][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0984 (0.0599)	
0.9999776 6.2334436e-08
loss:  0.04461853269148164 0.043041068380329284
===========>   training    <===========
Epoch: [296][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1263 (0.1263)	
0.9998975 3.052143e-09
===========>   testing    <===========
Epoch: [296][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0487 (0.0487)	
0.99996614 2.726351e-07
Epoch: [296][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0663 (0.0675)	
0.99973184 2.432255e-07
Epoch: [296][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0926 (0.0638)	
0.9999769 1.833621e-07
loss:  0.04860868502895843 0.043041068380329284
===========>   training    <===========
Epoch: [297][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1062 (0.1062)	
0.9999726 2.1902291e-07
===========>   testing    <===========
Epoch: [297][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0617 (0.0617)	
0.99996877 9.582302e-08
Epoch: [297][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.3464 (0.0785)	
0.9998659 1.4616701e-07
Epoch: [297][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1762 (0.0728)	
0.9999802 7.896874e-08
loss:  0.053435692072029806 0.043041068380329284
===========>   training    <===========
Epoch: [298][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1281 (0.1281)	
0.9999758 1.9777563e-07
===========>   testing    <===========
Epoch: [298][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0391 (0.0391)	
0.999972 1.0929607e-07
Epoch: [298][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0383 (0.0683)	
0.9999542 2.2262992e-07
Epoch: [298][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0837 (0.0621)	
0.9999802 1.0531127e-07
loss:  0.04660246360130571 0.043041068380329284
===========>   training    <===========
Epoch: [299][0/46]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.1190 (0.1190)	
0.99989605 6.144205e-08
===========>   testing    <===========
Epoch: [299][0/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0491 (0.0491)	
0.9999552 6.7805765e-08
Epoch: [299][100/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0741 (0.0628)	
0.99985397 1.2288675e-07
Epoch: [299][200/289]	Lr-deconv: [0.0]	Lr-other: [0.00048767497911552955]	Loss 0.0731 (0.0587)	
0.9999676 6.894597e-08
loss:  0.04506452709205533 0.043041068380329284
